{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0be0410",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Read-the-input-files\" data-toc-modified-id=\"Read-the-input-files-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Read the input files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-paths-to-data\" data-toc-modified-id=\"Set-paths-to-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Set paths to data</a></span></li><li><span><a href=\"#CEN-letters-set\" data-toc-modified-id=\"CEN-letters-set-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>CEN letters set</a></span></li><li><span><a href=\"#Garfagnini's-invenctory-(cleaned-OCR)\" data-toc-modified-id=\"Garfagnini's-invenctory-(cleaned-OCR)-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Garfagnini's invenctory (cleaned OCR)</a></span></li></ul></li><li><span><a href=\"#Prepare-the-input-files:-explode-multiple-correspondents-to-one-per-row\" data-toc-modified-id=\"Prepare-the-input-files:-explode-multiple-correspondents-to-one-per-row-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Prepare the input files: explode multiple correspondents to one per row</a></span><ul class=\"toc-item\"><li><span><a href=\"#CEN-letters-set\" data-toc-modified-id=\"CEN-letters-set-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>CEN letters set</a></span></li><li><span><a href=\"#Garfagnini's-set\" data-toc-modified-id=\"Garfagnini's-set-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Garfagnini's set</a></span></li></ul></li><li><span><a href=\"#Combine-Garfagnini-+-CEN-sets\" data-toc-modified-id=\"Combine-Garfagnini-+-CEN-sets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Combine Garfagnini + CEN sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set1:-Get-the-CEN-slice-(up-to-second-degree)-that-contains-the-mapped-names-from-Garfagnini's-set\" data-toc-modified-id=\"Set1:-Get-the-CEN-slice-(up-to-second-degree)-that-contains-the-mapped-names-from-Garfagnini's-set-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Set1: Get the CEN slice (up to second degree) that contains the mapped names from Garfagnini's set</a></span></li><li><span><a href=\"#Set2:-Get-the-intrinsic-slice-of-CEN-(up-to-2nd-degree)-of-Magliabechi\" data-toc-modified-id=\"Set2:-Get-the-intrinsic-slice-of-CEN-(up-to-2nd-degree)-of-Magliabechi-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Set2: Get the intrinsic slice of CEN (up to 2nd degree) of Magliabechi</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-First-degree-correspondents-for-every-person-in-CEN\" data-toc-modified-id=\"Generate-First-degree-correspondents-for-every-person-in-CEN-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Generate First-degree correspondents for every person in CEN</a></span></li><li><span><a href=\"#Indicate-person-of-interest\" data-toc-modified-id=\"Indicate-person-of-interest-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Indicate person of interest</a></span></li><li><span><a href=\"#Get-First-degree-correspondents-per-person-of-interest\" data-toc-modified-id=\"Get-First-degree-correspondents-per-person-of-interest-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Get First degree correspondents per person of interest</a></span></li><li><span><a href=\"#Add-a-mark-to-Letters-file-with-correspondents-of-person-of-interest\" data-toc-modified-id=\"Add-a-mark-to-Letters-file-with-correspondents-of-person-of-interest-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Add a mark to Letters file with correspondents of person of interest</a></span></li><li><span><a href=\"#Mark-the-slice\" data-toc-modified-id=\"Mark-the-slice-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>Mark the slice</a></span></li><li><span><a href=\"#Get-the-slice\" data-toc-modified-id=\"Get-the-slice-3.2.6\"><span class=\"toc-item-num\">3.2.6&nbsp;&nbsp;</span>Get the slice</a></span></li></ul></li><li><span><a href=\"#Resulting-CEN-subsets:-Combining-Set1-and-Set2\" data-toc-modified-id=\"Resulting-CEN-subsets:-Combining-Set1-and-Set2-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Resulting CEN subsets: Combining Set1 and Set2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remove-CEN-letter-duplicates\" data-toc-modified-id=\"Remove-CEN-letter-duplicates-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Remove CEN letter duplicates</a></span></li></ul></li><li><span><a href=\"#Resulting-dataset:-Combined-letters-Magliabechi's-second-degree-from-Garfagnini-and-CEN-datasets\" data-toc-modified-id=\"Resulting-dataset:-Combined-letters-Magliabechi's-second-degree-from-Garfagnini-and-CEN-datasets-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Resulting dataset: Combined letters Magliabechi's second-degree from Garfagnini and CEN datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Summary-counts\" data-toc-modified-id=\"Summary-counts-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Summary counts</a></span></li></ul></li></ul></li><li><span><a href=\"#Create-correspondent-pairs-and-their-correspondence-year-ranges\" data-toc-modified-id=\"Create-correspondent-pairs-and-their-correspondence-year-ranges-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create correspondent pairs and their correspondence year ranges</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-a-dictionary-of-personId-and-person-name\" data-toc-modified-id=\"Create-a-dictionary-of-personId-and-person-name-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Create a dictionary of personId and person name</a></span></li><li><span><a href=\"#Create-correspondent-pairs\" data-toc-modified-id=\"Create-correspondent-pairs-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Create correspondent pairs</a></span></li><li><span><a href=\"#Create-letter-year-ranges-for-each-pair\" data-toc-modified-id=\"Create-letter-year-ranges-for-each-pair-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Create letter year ranges for each pair</a></span><ul class=\"toc-item\"><li><span><a href=\"#For-pairs-with-letter-years-in-slice1_certain\" data-toc-modified-id=\"For-pairs-with-letter-years-in-slice1_certain-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>For pairs with letter years in slice1_certain</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-subset\" data-toc-modified-id=\"Create-subset-4.3.1.1\"><span class=\"toc-item-num\">4.3.1.1&nbsp;&nbsp;</span>Create subset</a></span></li><li><span><a href=\"#Get-the-correspondence-year-ranges\" data-toc-modified-id=\"Get-the-correspondence-year-ranges-4.3.1.2\"><span class=\"toc-item-num\">4.3.1.2&nbsp;&nbsp;</span>Get the correspondence year ranges</a></span></li><li><span><a href=\"#Add-these-years-to-letters-file-from-this-group\" data-toc-modified-id=\"Add-these-years-to-letters-file-from-this-group-4.3.1.3\"><span class=\"toc-item-num\">4.3.1.3&nbsp;&nbsp;</span>Add these years to letters file from this group</a></span></li></ul></li><li><span><a href=\"#For-pairs-with-letter-years-in-slice2_uncertain\" data-toc-modified-id=\"For-pairs-with-letter-years-in-slice2_uncertain-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>For pairs with letter years in slice2_uncertain</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-subset\" data-toc-modified-id=\"Create-subset-4.3.2.1\"><span class=\"toc-item-num\">4.3.2.1&nbsp;&nbsp;</span>Create subset</a></span></li><li><span><a href=\"#Split-dates-of-birth/death/fl-of-correspondents\" data-toc-modified-id=\"Split-dates-of-birth/death/fl-of-correspondents-4.3.2.2\"><span class=\"toc-item-num\">4.3.2.2&nbsp;&nbsp;</span>Split dates of birth/death/fl of correspondents</a></span></li><li><span><a href=\"#Assign-min-and-max-year-of-correspondence-exchange-using-dates-of-birth/death/fl-of-correspondents\" data-toc-modified-id=\"Assign-min-and-max-year-of-correspondence-exchange-using-dates-of-birth/death/fl-of-correspondents-4.3.2.3\"><span class=\"toc-item-num\">4.3.2.3&nbsp;&nbsp;</span>Assign min and max year of correspondence exchange using dates of birth/death/fl of correspondents</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Prepare-final-unique-correspondent-pairs-dataset\" data-toc-modified-id=\"Prepare-final-unique-correspondent-pairs-dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Prepare final unique correspondent pairs dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Combine-slice2_uncertain-with-slice1_certain\" data-toc-modified-id=\"Combine-slice2_uncertain-with-slice1_certain-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Combine slice2_uncertain with slice1_certain</a></span></li><li><span><a href=\"#Drop-duplicates\" data-toc-modified-id=\"Drop-duplicates-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Drop duplicates</a></span></li><li><span><a href=\"#Split-dates-of-birth/date/fl-of-correspondents\" data-toc-modified-id=\"Split-dates-of-birth/date/fl-of-correspondents-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Split dates of birth/date/fl of correspondents</a></span></li><li><span><a href=\"#Find-earliest-date-of-death-of-the-correspondents-(pending-to-complete)\" data-toc-modified-id=\"Find-earliest-date-of-death-of-the-correspondents-(pending-to-complete)-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Find earliest date of death of the correspondents (pending to complete)</a></span></li><li><span><a href=\"#Change-column-names\" data-toc-modified-id=\"Change-column-names-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Change column names</a></span></li><li><span><a href=\"#Resulting-dataset:-correspondent-pairs\" data-toc-modified-id=\"Resulting-dataset:-correspondent-pairs-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Resulting dataset: correspondent pairs</a></span></li></ul></li><li><span><a href=\"#Read-and-integrate-extra-data:-annotated-ties\" data-toc-modified-id=\"Read-and-integrate-extra-data:-annotated-ties-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Read and integrate extra data: annotated ties</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extra-dataA:-annotated-NEGATIVE-ties-for-previous-groups\" data-toc-modified-id=\"Extra-dataA:-annotated-NEGATIVE-ties-for-previous-groups-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Extra dataA: annotated NEGATIVE ties for previous groups</a></span><ul class=\"toc-item\"><li><span><a href=\"#Upload-dataset-A\" data-toc-modified-id=\"Upload-dataset-A-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Upload dataset A</a></span></li><li><span><a href=\"#Integrate-datasetA-data-with-existing-groups\" data-toc-modified-id=\"Integrate-datasetA-data-with-existing-groups-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Integrate datasetA data with existing groups</a></span><ul class=\"toc-item\"><li><span><a href=\"#Add-end-of-conflict-year-to-annotated-dataset-using-rules\" data-toc-modified-id=\"Add-end-of-conflict-year-to-annotated-dataset-using-rules-6.1.2.1\"><span class=\"toc-item-num\">6.1.2.1&nbsp;&nbsp;</span>Add end of conflict year to annotated dataset using rules</a></span></li><li><span><a href=\"#Generate-relations-dataset-depending-on-the-case\" data-toc-modified-id=\"Generate-relations-dataset-depending-on-the-case-6.1.2.2\"><span class=\"toc-item-num\">6.1.2.2&nbsp;&nbsp;</span>Generate relations dataset depending on the case</a></span></li></ul></li></ul></li><li><span><a href=\"#Extra-dataB:-annotated-ties-for-non-existing-pairs\" data-toc-modified-id=\"Extra-dataB:-annotated-ties-for-non-existing-pairs-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Extra dataB: annotated ties for non-existing pairs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Upload-dataset-B\" data-toc-modified-id=\"Upload-dataset-B-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Upload dataset B</a></span></li><li><span><a href=\"#Generate-correspondent-pair-Ids\" data-toc-modified-id=\"Generate-correspondent-pair-Ids-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Generate correspondent pair Ids</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-personStrId\" data-toc-modified-id=\"Create-personStrId-6.2.2.1\"><span class=\"toc-item-num\">6.2.2.1&nbsp;&nbsp;</span>Create personStrId</a></span></li><li><span><a href=\"#Create-a-dictionary-of-personId-and-person-name\" data-toc-modified-id=\"Create-a-dictionary-of-personId-and-person-name-6.2.2.2\"><span class=\"toc-item-num\">6.2.2.2&nbsp;&nbsp;</span>Create a dictionary of personId and person name</a></span></li><li><span><a href=\"#Generate-pair-Ids\" data-toc-modified-id=\"Generate-pair-Ids-6.2.2.3\"><span class=\"toc-item-num\">6.2.2.3&nbsp;&nbsp;</span>Generate pair Ids</a></span></li></ul></li><li><span><a href=\"#Add-end-of-conflict-year-to-annotated-dataset-using-rules\" data-toc-modified-id=\"Add-end-of-conflict-year-to-annotated-dataset-using-rules-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Add end of conflict year to annotated dataset using rules</a></span></li></ul></li></ul></li><li><span><a href=\"#Combine-all-relations-into-one-set\" data-toc-modified-id=\"Combine-all-relations-into-one-set-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Combine all relations into one set</a></span></li><li><span><a href=\"#Create-slices-per-year\" data-toc-modified-id=\"Create-slices-per-year-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Create slices per year</a></span><ul class=\"toc-item\"><li><span><a href=\"#Certain-slice-only\" data-toc-modified-id=\"Certain-slice-only-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Certain slice only</a></span></li><li><span><a href=\"#ALL:-including-uncertain\" data-toc-modified-id=\"ALL:-including-uncertain-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>ALL: including uncertain</a></span></li></ul></li><li><span><a href=\"#ANALYSIS-(R-script)\" data-toc-modified-id=\"ANALYSIS-(R-script)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>ANALYSIS (R script)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f38e7",
   "metadata": {},
   "source": [
    "This notebook describes all the processes used in the article \"Unlinked: Conflict Management and Structural Balance in the Early Modern Republic of Letters\" by Ingeborg van Vugt and Liliana Melgar (DOI: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a452f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import csv\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "# to add timestamp to file names\n",
    "import time\n",
    "\n",
    "# This library is needed to convert enclosed strings in a list to a real list (https://www.youtube.com/watch?v=t8kAiabrM_E)\n",
    "from ast import literal_eval\n",
    "\n",
    "# import os.path to add paths to files\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce5781d",
   "metadata": {},
   "source": [
    "# Read the input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102f32e",
   "metadata": {},
   "source": [
    "## Set paths to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to repository: add the path to the folder you will use for this project in your local machine.\n",
    "\n",
    "# path to the main data folder in the repository\n",
    "data_directory = os.path.abspath(os.path.join('..', 'data'))\n",
    "# path to the sub-folder in the repository where the raw (unprocessed) data is located\n",
    "data_raw_directory = os.path.join(data_directory, 'raw')\n",
    "# path to the sub-folder in the repository where the processed data is located\n",
    "data_processed_directory = os.path.join(data_directory, 'processed')\n",
    "# path to the sub-folder in the repository to store temporary data files\n",
    "data_temp_directory = os.path.join(data_directory, 'temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1cc7d",
   "metadata": {},
   "source": [
    "## CEN letters set\n",
    "We upload the entire CEN dataset which is being curated by SKILLNET for the years 1200-1820. For this article, we use the cleaned version of the CEN (latest version is to find here (detailed citation is added to the article): https://dataverse.nl/dataset.xhtml?persistentId=doi:10.34894/G8XQI0), which is stored in the data/raw folder of the article's repository. In this link you can also see an explanation of the meaning of each column and the data cleaning operations applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f62d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CEN letters file\n",
    "letters_path = os.path.join(data_raw_directory, 'CEN_Letters_version_20220414.csv')\n",
    "\n",
    "cen_letters_imp = pd.read_csv(letters_path, sep = \",\", index_col=False, engine='python')\n",
    "\n",
    "# convert datatypes and fill in empty values\n",
    "\n",
    "# convert and fill in depending on original data type\n",
    "cen_letters_imp_columns = cen_letters_imp.columns\n",
    "for column in cen_letters_imp_columns:\n",
    "    dataType = cen_letters_imp.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        cen_letters_imp[column] = cen_letters_imp[column].fillna(0.0)\n",
    "        cen_letters_imp[column] = cen_letters_imp[column].astype(int)\n",
    "    if dataType == object:\n",
    "        cen_letters_imp[column] = cen_letters_imp[column].fillna('null')\n",
    "        cen_letters_imp[column] = cen_letters_imp[column].astype(str)\n",
    "        \n",
    "# make a copy\n",
    "letV01_v00 = cen_letters_imp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b154a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# letV01_v00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e80b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only necessary columns for the article\n",
    "\n",
    "letV01 = letV01_v00[['SKletterId', \n",
    "                     'SKpersonId_sender',\n",
    "                     'personStrId_sender',\n",
    "                     'magliaOCRPersonId_sender',\n",
    "                     'SKpersonId_receiver',\n",
    "                     'personStrId_receiver',\n",
    "                     'magliaOCRPersonId_receiver',\n",
    "                     'YEAR_Original',\n",
    "                     'yearItem_start',\n",
    "                     'yearItem_end',\n",
    "                     'yearItemSk',\n",
    "                     'isYearRange',\n",
    "                     'isYearRangeInferredSk',\n",
    "                     'isYearRangeUncertain',\n",
    "                     'isYearItemInferredSk',\n",
    "                     'isYearItemUncertain',\n",
    "                     'Aantal_CleanedSk',\n",
    "                     'URLOrLocator',\n",
    "                     'ISBD_OriginalCEN']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f762152",
   "metadata": {},
   "outputs": [],
   "source": [
    "letV01.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ace11c",
   "metadata": {},
   "source": [
    "## Garfagnini's invenctory (cleaned OCR)\n",
    "We upload the curated dataset of Garfagnini's inventory of Magliabechi's letters. For this article we included the dated letters only. For more information see: https://dataverse.nl/dataset.xhtml?persistentId=doi:10.34894/GTV2RN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b190a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Garfagnini's letters file\n",
    "letters_path = os.path.join(data_raw_directory, 'garfagniniMagliabechiOCR_version_20220414.csv') # change to: garfagniniMagliabechiOCR_version20221115.csv\n",
    "\n",
    "garfag_letters_imp = pd.read_csv(letters_path, sep = \"â‘„\", index_col=False, engine='python') # change to ,\n",
    "\n",
    "garfag_letters_imp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c20d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only necessary columns for the article\n",
    "\n",
    "garfV01_v00 = garfag_letters_imp[[\n",
    "'SKletterId',\n",
    "'SKpersonId_sender',\n",
    "'personStrId_sender',\n",
    "'SKpersonId_receiver',\n",
    "'personStrId_receiver',\n",
    "'YEAR_Original',\n",
    "'yearItem_start',\n",
    "'yearItem_end',\n",
    "'yearItemSk',\n",
    "'isYearRange',\n",
    "'isYearRangeInferredSk',\n",
    "'isYearRangeUncertain',\n",
    "'isYearItemInferredSk',\n",
    "'isYearItemUncertain',\n",
    "'URLOrLocator',\n",
    "'titleOriginal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "garfV01_v00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58556411",
   "metadata": {},
   "outputs": [],
   "source": [
    "garfV01_v01 = garfV01_v00.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# garfV01_v01.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "garfV01_v01.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d66321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test, here add Id of a letter I know has multiple senders/receivers\n",
    "# test1 = garfV01_v01[garfV01_v01.SKletterId.str.contains('letOcr_011612')]\n",
    "# # test1 = garfV01_v01[garfV01_v01.SKletterId.str.contains('letOcr_006148')]\n",
    "\n",
    "# #skp025230\n",
    "\n",
    "# test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# test100 = garfV01_v01[garfV01_v01.SKpersonId_sender.str.contains('mop1815')]\n",
    "\n",
    "# test100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a45c6",
   "metadata": {},
   "source": [
    "# Prepare the input files: explode multiple correspondents to one per row\n",
    "In this version of the CEN and Garfagnini's set, there were occassionally multiple correspondents per row (i.e., a letter that was sent to or by many persons). For network analysis, we need to have every single person in a separate row. The next steps do this split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55722401",
   "metadata": {},
   "source": [
    "## CEN letters set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd8343",
   "metadata": {},
   "source": [
    "Not applicable, the CEN at this stage (March 21, 2022) doesn't have yet multiple correspondents as senders or receivers. They do exist, but at this moment they are still in the original columns (afzender/ontvanger), and the current cleaned columns only contain one of them per row. Because this article doesn't use letters in which there are multiple correspondents in CEN, the data won't be updated and thus the code is not applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e6e6f",
   "metadata": {},
   "source": [
    "## Garfagnini's set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a85633",
   "metadata": {},
   "source": [
    "This set does have multiple senders in one cell. Thus, it is necessary to separate them in a way that each person is in one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make first a subset of the letters that have multiple senders (there are no letters with multiple receivers in this set)\n",
    "garfV01_v02 = garfV01_v01[garfV01_v01.SKpersonId_sender.str.contains(',') == True]\n",
    "garfV01_v02.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20349a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "garfV01_v02.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43050a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# garfV01_v02.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79588fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a dataset of the letters that DO NOT have multiple senders\n",
    "garfV01_v03 = garfV01_v01[garfV01_v01.SKpersonId_sender.str.contains(',') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "garfV01_v03.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test, here add Id of a letter I know has multiple senders/receivers and shoulnd't be in this set\n",
    "# test01 = garfV01_v03[garfV01_v03.SKletterId.str.contains('letOcr_011612')]\n",
    "# #skp025230\n",
    "\n",
    "# test01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the copy that contains multiple senders\n",
    "garfV01_v04 = garfV01_v02.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the copy that DOESN'T contain multiple senders\n",
    "garfV01_v05 = garfV01_v03.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# garfV01_v04.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# garfV01_v05.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2338d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because there are only multiple senders and not multiple receivers, we convert to list only the senders. The purpose of converting it to a list is to facilitate the splitting into multiple rows (one per person)\n",
    "# literal_eval is a library that is needed to convert enclosed strings in a list to a real list (https://www.youtube.com/watch?v=t8kAiabrM_E)\n",
    "garfV01_v04['SKpersonId_sender'] = garfV01_v04['SKpersonId_sender'].apply(literal_eval) #convert to list type\n",
    "garfV01_v04['personStrId_sender'] = garfV01_v04['personStrId_sender'].apply(literal_eval) #convert to list type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28855254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now \"explode\" the multiple senders into different rows, one row per person (keeping all the remaining metadata as the same)\n",
    "garfV01_v06 = garfV01_v04.apply(pd.Series.explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123fa957",
   "metadata": {},
   "outputs": [],
   "source": [
    "garfV01_v06.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "garfV01_v07 = garfV01_v06.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbeea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "garfV01_v07.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test, here add Id of a letter I know has multiple senders/receivers\n",
    "# test02 = garfV01_v07[garfV01_v07.SKletterId.str.contains('letOcr_020003')]\n",
    "# #letOcr_011612\n",
    "\n",
    "# test02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine this set with the other letters that didn't have multiple senders\n",
    "garfV01_v08 = pd.concat([garfV01_v07, garfV01_v05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a712962",
   "metadata": {},
   "outputs": [],
   "source": [
    "garfV01_v08.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of letters\n",
    "garfV01_v08.SKletterId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b030ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test, here add Id of a letter I know has multiple senders/receivers\n",
    "# test6 = garfV01_v08[garfV01_v08.SKletterId.str.contains('letOcr_011612')]\n",
    "# #skp025230\n",
    "\n",
    "# test6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Garfagnini's set prepared (1 row per person when there were multiple correspondents per letter)\n",
    "garfV02 = garfV01_v08.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "garfV02.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2927b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test, here add Id of a letter I know has multiple senders/receivers\n",
    "# test03 = garfV02[garfV02.SKletterId.str.contains('letOcr_020003')]\n",
    "# # letOcr_011612\n",
    "\n",
    "# test03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3071f",
   "metadata": {},
   "source": [
    "# Combine Garfagnini + CEN sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d45af",
   "metadata": {},
   "source": [
    "In this step, we capture the different sets and combine them into one.\n",
    "1. First, we get the subsets from the CEN:\n",
    "- Set1: This is the slice of the CEN dataset which contains the second-degree network of all the mapped names from Garfagnini's set (first-degree)\n",
    "- Set2: This is the slice of the CEN dataset which contains the second-degree network of Magliabechi (intrinsic to the CEN)\n",
    "- Resulting CEN subsets: This is the combination of Set1 and Set2 removing duplicates.\n",
    "2. Then, we combine this resulting CEN subsets with the entire Garfagnini's set uploaded before\n",
    "- In step 3.4 this resulting set is stored, it can also be downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9456f1",
   "metadata": {},
   "source": [
    "## Set1: Get the CEN slice (up to second degree) that contains the mapped names from Garfagnini's set\n",
    "All person names in Garfagnini's set were mapped to the CEN person names. When there was a correct mapping we added a \"y\" mark to a column \"magliaOCRPersonId_sender\" and/or \"magliaOCRPersonId_receiver\".\n",
    "Here we capture the slice of the letters (rows) that have this mark in either sender OR receiver, thus capturing the correspondents up to a second-degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffa172",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENslice2ndGarf_v00 = letV01[letV01.magliaOCRPersonId_sender.str.contains('y') | letV01.magliaOCRPersonId_receiver.str.contains('y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENslice2ndGarf_v00.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc292ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "CENslice2ndGarf_v01 = CENslice2ndGarf_v00.drop(['magliaOCRPersonId_sender', \n",
    "                                                'magliaOCRPersonId_receiver'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENslice2ndGarf = CENslice2ndGarf_v01.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab83d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENslice2ndGarf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65847e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store this file externally\n",
    "\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# fileName1 = (f\"{pathFolder}set1_CEN2ndDGarfagnini_slice_Letters_{timestr}.csv\")\n",
    "# CENslice2ndGarf.to_csv(fileName1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc678e",
   "metadata": {},
   "source": [
    "## Set2: Get the intrinsic slice of CEN (up to 2nd degree) of Magliabechi\n",
    "The CEN dataset contains a few first-degree correspondents of Magliabechi. For each first-degree correspondent, we capture here the correspondents (i.e., the second-degree network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d91749",
   "metadata": {},
   "source": [
    "### Generate First-degree correspondents for every person in CEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e29c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group ontvangers per afzender\n",
    "\n",
    "# following instructions from this site, but adding \"unique\": https://towardsdatascience.com/10-python-pandas-tricks-to-make-data-analysis-more-enjoyable-cb8f55af8c30\n",
    "letV01_ontPerAfz_v01 = letV01.groupby('SKpersonId_sender')['SKpersonId_receiver'].unique()\\\n",
    "                             .apply(list)\\\n",
    "                             .reset_index()\n",
    "\n",
    "# Rename columns\n",
    "letV01_ontPerAfz_v01.rename(columns={\"SKpersonId_sender\":\"SKpersonId\"},inplace=True)\n",
    "letV01_ontPerAfz_v01.rename(columns={\"SKpersonId_receiver\":\"Correspondents\"},inplace=True)\n",
    "\n",
    "# apply function to convert the list to strings separated by '|'\n",
    "letV01_ontPerAfz_v01['Correspondents'] = letV01_ontPerAfz_v01.apply(lambda x: ('|'.join([str(s) for s in x['Correspondents']])), axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "## Group afzenders per ontvanger\n",
    "\n",
    "# following instructions from this site, but adding \"unique\": https://towardsdatascience.com/10-python-pandas-tricks-to-make-data-analysis-more-enjoyable-cb8f55af8c30\n",
    "letV01_afzPerOnt_v01 = letV01.groupby('SKpersonId_receiver')['SKpersonId_sender'].unique()\\\n",
    "                             .apply(list)\\\n",
    "                             .reset_index()\n",
    "\n",
    "# rename columns\n",
    "letV01_afzPerOnt_v01.rename(columns={\"SKpersonId_receiver\":\"SKpersonId\"},inplace=True)\n",
    "letV01_afzPerOnt_v01.rename(columns={\"SKpersonId_sender\":\"Correspondents\"},inplace=True)\n",
    "\n",
    "# apply function to convert the list to strings separated by '|'\n",
    "letV01_afzPerOnt_v01['Correspondents'] = letV01_afzPerOnt_v01.apply(lambda x: ('|'.join([str(s) for s in x['Correspondents']])), axis = 1)\n",
    "\n",
    "\n",
    "## Put the two dataframes together\n",
    "corrV01_v04 = letV01_ontPerAfz_v01.append(letV01_afzPerOnt_v01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I have to stack it, because otherwise it won't detect unique values, since now they are joined by |\n",
    "# We start with creating a new dataframe from the series with personId as the index\n",
    "corrV01_v05 = pd.DataFrame(corrV01_v04.Correspondents.str.split('|').tolist(), index=corrV01_v04.SKpersonId).stack()\n",
    "\n",
    "# We now want to get rid of the secondary index\n",
    "# To do this, we will make EmployeeId as a column (it can't be an index since the values will be duplicate)\n",
    "corrV01_v05 = corrV01_v05.reset_index([0, 'SKpersonId'])\n",
    "\n",
    "# Set the column names as we want them\n",
    "corrV01_v05.columns = ['SKpersonId', 'Correspondents']\n",
    "\n",
    "# Remove duplicates (leave unique values only)\n",
    "corrV01_v06 = corrV01_v05[['SKpersonId', 'Correspondents']].drop_duplicates()\\\n",
    "                                                           .groupby('SKpersonId')['Correspondents'].unique()\\\n",
    "                                                           .apply(list)\\\n",
    "                                                           .reset_index()\n",
    "\n",
    "# apply function to convert the list to strings separated by '|'\n",
    "corrV01_v06['Correspondents'] = corrV01_v06.apply(lambda x: ('|'.join([str(s) for s in x['Correspondents']])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST to see if the Ids of the first-degree correspondents of the person of interest are captured\n",
    "# test03 = corrV01_v06[corrV01_v06.SKpersonId == 'skp003623'] #Magliabechi\n",
    "\n",
    "# display(HTML(test03.head().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE ARE FIRST DEGREE: a dataframe that contains all persons and their first degree correspondents (with SKpersonID)\n",
    "corrV01_1stBySKpId = corrV01_v06.set_index('SKpersonId')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec085bfb",
   "metadata": {},
   "source": [
    "### Indicate person of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable personOfInterest\n",
    "\n",
    "personOfInterest = 'skp003623' #magliabechi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9241da",
   "metadata": {},
   "source": [
    "### Get First degree correspondents per person of interest\n",
    "(returns list of SKpIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the cell where the first degree correspondents of a person of interest are included \n",
    "# (using the dataframe correspondents1stDegreeB which contains the list with SKpIds)\n",
    "# this retuns a series object\n",
    "corrV01_1stBySKpIdPerPers_v01 = corrV01_1stBySKpId.loc[personOfInterest]\n",
    "\n",
    "# split correspondents\n",
    "corrV01_1stBySKpIdPerPers_v02 = corrV01_1stBySKpIdPerPers_v01.str.split(\"|\")\n",
    "\n",
    "# converting series object to list\n",
    "corrV01_1stBySKpIdPerPers_v03 = corrV01_1stBySKpIdPerPers_v02.to_list()\n",
    "\n",
    "# get unique values only\n",
    "corrV01_1stBySKpIdPerPers_v04 = np.unique(corrV01_1stBySKpIdPerPers_v03)\n",
    "\n",
    "# convert array with first-degree correspondents to a list\n",
    "corrV01_1stBySKpIdPerPers = corrV01_1stBySKpIdPerPers_v04.tolist()\n",
    "\n",
    "# Notes to understand why these are different:\n",
    "# firstDegreeCorrespPerPersonList (corrV01_1stBySKpIdPerPers_v03) --> is an object of type list, but it comes in double square brackets: [['skpTEST', 'skp038427', 'skp003623', 'skp011706', 'skp035864']]\n",
    "# firstDegreeCorrespPerPersonUniqueList (corrV01_1stBySKpIdPerPers) --> is also an object of type list, but it comes in single square brackets: ['skp003623', 'skp011706', 'skp035864', 'skp038427', 'skpTEST']\n",
    "# if I understand correctly, the first one (double square brackets) it's still a subset of the dataframe, that's why I can call info using those values, while the second one is a flat list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93212da",
   "metadata": {},
   "source": [
    "### Add a mark to Letters file with correspondents of person of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "letV01_withSliceMark = letV01.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Senders\n",
    "\n",
    "# Identify which senders are in the list of 1st degree correspondents of the person of interest and mark them\n",
    "letV01_withSliceMark['isSliceOfPersOfInt_afz'] = letV01_withSliceMark.SKpersonId_sender[letV01_withSliceMark.SKpersonId_sender.isin(corrV01_1stBySKpIdPerPers)].copy()\n",
    "\n",
    "letV01_withSliceMark.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill empty values\n",
    "letV01_withSliceMark['isSliceOfPersOfInt_afz'] = letV01_withSliceMark['isSliceOfPersOfInt_afz'].fillna('null').copy()\n",
    "letV01_withSliceMark['isSliceOfPersOfInt_afz'] = letV01_withSliceMark['isSliceOfPersOfInt_afz'].astype('string').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the value in the column \"isSliceOfPersOfInt\" from the afzender Id to \"y_afz\"\n",
    "letV01_withSliceMark['isSliceOfPersOfInt_afz'] = letV01_withSliceMark['isSliceOfPersOfInt_afz'].replace(to_replace=r'\\d.*', value='y_afz', regex=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44940d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receivers\n",
    "\n",
    "# Identify which senders are in the list of 1st degree correspondents of the person of interest and mark them\n",
    "letV01_withSliceMark['isSliceOfPersOfInt_ont'] = letV01_withSliceMark.SKpersonId_receiver[letV01_withSliceMark.SKpersonId_receiver.isin(corrV01_1stBySKpIdPerPers)].copy()\n",
    "\n",
    "letV01_withSliceMark.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill empty values\n",
    "letV01_withSliceMark['isSliceOfPersOfInt_ont'] = letV01_withSliceMark['isSliceOfPersOfInt_ont'].fillna('null').copy()\n",
    "letV01_withSliceMark['isSliceOfPersOfInt_ont'] = letV01_withSliceMark['isSliceOfPersOfInt_ont'].astype('string').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the value in the column \"isSliceOfPersOfInt\" from the afzender Id to \"y_ont\"\n",
    "letV01_withSliceMark['isSliceOfPersOfInt_ont'] = letV01_withSliceMark['isSliceOfPersOfInt_ont'].replace(to_replace=r'\\d.*', value='y_ont', regex=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "letV01_withSliceMark['isSliceOfPersOfInt_ont'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caebc6",
   "metadata": {},
   "source": [
    "### Mark the slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ee96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "letV01_withSliceMark['isSliceOfPersOfInt'] = letV01_withSliceMark['isSliceOfPersOfInt_afz'] + '-' + letV01_withSliceMark['isSliceOfPersOfInt_ont']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "letV01_withSliceMark['isSliceOfPersOfInt'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d683e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values to leave only y if it is part of the slice\n",
    "letV01_withSliceMark['isSliceOfPersOfInt'] = letV01_withSliceMark['isSliceOfPersOfInt'].replace(to_replace=r'y.*|.*y.*', value='y', regex=True).copy()\n",
    "\n",
    "# replace null-null for null\n",
    "letV01_withSliceMark['isSliceOfPersOfInt'] = letV01_withSliceMark['isSliceOfPersOfInt'].replace(to_replace=r'null-null', value='null', regex=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE FINAL VERSION OF LETTERS THAT CONTAINS MARKS OF THE UP TO 2ND DEGREE CORRESP OF A PERSON OF INTEREST\n",
    "letV01_withSliceMarkFinal = letV01_withSliceMark.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "letV01_withSliceMarkFinal['isSliceOfPersOfInt'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425cdfc",
   "metadata": {},
   "source": [
    "### Get the slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceCENLettersMaglia2ndD_v01 = letV01_withSliceMarkFinal[letV01_withSliceMarkFinal.isSliceOfPersOfInt.str.contains('y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceCENLettersMaglia2ndD_v01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4db6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceCENLettersMaglia2ndD_v01.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceCENLettersMaglia2ndD_v01.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a99a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns with marks\n",
    "sliceCENLettersMaglia2ndD = sliceCENLettersMaglia2ndD_v01.drop(['isSliceOfPersOfInt_afz', \n",
    "                                                                'isSliceOfPersOfInt_ont', \n",
    "                                                                'isSliceOfPersOfInt',\n",
    "                                                                'magliaOCRPersonId_sender',\n",
    "                                                                'magliaOCRPersonId_receiver'], axis=1).reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01412d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceCENLettersMaglia2ndD.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae3cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store this file externally\n",
    "\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# fileName2 = (f\"{pathFolder}set3_CENMagliabechiUpTo2ndDegree_slice_Letters_{timestr}.csv\")\n",
    "# sliceCENLettersMaglia2ndD.to_csv(fileName2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a64d4",
   "metadata": {},
   "source": [
    "## Resulting CEN subsets: Combining Set1 and Set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply combine Set1 and Set2 (both from CEN)\n",
    "\n",
    "combinedCENsubsets_v00 = pd.concat((CENslice2ndGarf, sliceCENLettersMaglia2ndD), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5f0e9",
   "metadata": {},
   "source": [
    "### Remove CEN letter duplicates\n",
    "Between Set1 and Set2 there are duplicated records. Because the CEN dataset doesn't have at this point (March 2022) multiple senders or receivers, duplicates can be detected by just looking at the LetterId. If at some point the cleaning of the CEN integrates multiple senders/receivers, this step has to be removed or adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This removes duplicates using the LetterID but we still don't know if there are duplicated letters between Garfagnini's set and the CEN, because this requires letter mapping (to be done later, not so important for the article since\n",
    "# the calculations are not done using the number of letters, but only the years)\n",
    "\n",
    "combinedCENsubsets_v00.sort_values('SKletterId', inplace=True)\n",
    "combinedCENsubsets_v00.drop_duplicates('SKletterId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2544ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedCENsubsets = combinedCENsubsets_v00.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedCENsubsets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store this file externally\n",
    "\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# fileName3 = (f\"{pathRepository}set3_CENMagliabechiUpTo2ndDegree_slice_Letters_{timestr}.csv\")\n",
    "# combinedCENsubsets.to_csv(fileName3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4193381",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkNoLettersSlice2CenOnly = combinedCENsubsets[((combinedCENsubsets.isYearRange == 'n') & (combinedCENsubsets.isYearItemUncertain == 'y')) | ((combinedCENsubsets.isYearRangeUncertain == 'y') & (combinedCENsubsets.isYearItemUncertain == 'y')) | ((combinedCENsubsets.isYearRangeUncertain == 'y') & (combinedCENsubsets.isYearItemUncertain == 'n.a.'))]\n",
    "checkNoLettersSlice2CenOnly.SKletterId.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec2b769",
   "metadata": {},
   "source": [
    "## Resulting dataset: Combined letters Magliabechi's second-degree from Garfagnini and CEN datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine set3 (CEN sets) and Garfagnini's prepared set\n",
    "combinedSetGarfCEN_v00 = pd.concat((garfV02, combinedCENsubsets), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGarfCEN_v00.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGarfCEN_v00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "\n",
    "combinedSetGarfCEN_v00_columns = combinedSetGarfCEN_v00.columns\n",
    "\n",
    "for column in combinedSetGarfCEN_v00_columns:\n",
    "    dataType = combinedSetGarfCEN_v00.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        combinedSetGarfCEN_v00[column] = combinedSetGarfCEN_v00[column].fillna(0.0)\n",
    "        combinedSetGarfCEN_v00[column] = combinedSetGarfCEN_v00[column].astype(int)\n",
    "    if dataType == object:\n",
    "        combinedSetGarfCEN_v00[column] = combinedSetGarfCEN_v00[column].fillna('null')\n",
    "        combinedSetGarfCEN_v00[column] = combinedSetGarfCEN_v00[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGarfCEN_v00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test, here add Id of a letter I know has multiple senders/receivers\n",
    "# test05 = combinedSetGarfCEN_v00[combinedSetGarfCEN_v00.SKletterId.str.contains('letOcr_011612')]\n",
    "# #skp025230\n",
    "\n",
    "# test05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGarfCEN = combinedSetGarfCEN_v00.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGarfCEN.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c61636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test, here add Id of a letter I know has multiple senders/receivers\n",
    "# test500 = combinedSetGarfCEN[combinedSetGarfCEN.personStrId_sender.str.contains('_Unidentified_017690')]\n",
    "# #skp025230\n",
    "\n",
    "# test500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20483599",
   "metadata": {},
   "source": [
    "### Summary counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09058da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total number of letters\n",
    "countTotalLetters = combinedSetGarfCEN.SKletterId.nunique()\n",
    "countTotalLetters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of letters of Gargagnini's set\n",
    "countGarfagLetters = combinedSetGarfCEN[combinedSetGarfCEN.SKletterId.str.contains('letOcr')]\n",
    "countGarfagLetters.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6385e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of unique persons in Garfagnini's set (remember that only senders are different, receiver is only Magliabechi (except in one single case))\n",
    "countGarfagPersons = countGarfagLetters.SKpersonId_sender.nunique()\n",
    "countGarfagPersons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of unique persons in the combined dataset: See step 4.1 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa894302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store this set as a file in /data/temp folder of the repository\n",
    "\n",
    "# this line will add the time stamp to the file name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# (this is the resulting aggregated file of All Magliabechi's letters up to second-degree from the combination of Garfagnini's dataset and the CEN dataset)\n",
    "setGarfCEN_file = (f\"resultingDataset_CombinedGarfagniniAndCEN_Letters_{timestr}.csv\")\n",
    "setGarfCEN_path = os.path.join(data_temp_directory, setGarfCEN_file)\n",
    "combinedSetGarfCEN.to_csv(setGarfCEN_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa15d9",
   "metadata": {},
   "source": [
    "# Create correspondent pairs and their correspondence year ranges\n",
    "\n",
    "This section creates groups of correspondents. For example, Negri Salomon^0^0^1709 (sender) + Policala Georg^0^0^1709 (receiver) and Policala Georg^0^0^1709 (sender) + Negri Salomon^0^0^1709 (receiver) are converted into a single group with no roles (sender or receiver) but only taking them in alphabetical order. Thus: \n",
    "group# --> Negri Salomon^0^0^1709 + Policala Georg^0^0^1709\n",
    "\n",
    "In order to generate the combinations, multiple correspondents per letter were separated into different rows in previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3785b",
   "metadata": {},
   "source": [
    "## Create a dictionary of personId and person name\n",
    "For the resulting dataset, to be used in the group Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856103f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create dictionary for senders (source: https://cmdlinetips.com/2021/04/convert-two-column-values-from-pandas-dataframe-to-a-dictionary/)\n",
    "senders = combinedSetGarfCEN.set_index('SKpersonId_sender').to_dict()['personStrId_sender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15732f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# senders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in senders.items():\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(senders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2bceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test: find Id per value using list comprehension\n",
    "# dict_items = senders.items()\n",
    "# myValue1 = \"_Unidentified_017690^0^0^1687\"\n",
    "# myKey1 = [key for key,value in dict_items if value==myValue1]\n",
    "# print(myKey1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for receivers\n",
    "receivers = combinedSetGarfCEN.set_index('SKpersonId_receiver').to_dict()['personStrId_receiver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320851ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# receivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7887974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test: find Id per value using list comprehension\n",
    "# dict_items = receivers.items()\n",
    "# myValue2 = \"Johannes d' Outrein^1662^1722^0\"\n",
    "# myKey2 = [key for key,value in dict_items if value==myValue2]\n",
    "# print(myKey2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627446a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(receivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9440c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both dictionaries (source: https://datagy.io/python-merge-dictionaries/)\n",
    "personsDict = {**senders, **receivers} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e83808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique persons in the combined dictionary (-> number of unique persons in the resulting combined dataset)\n",
    "len(personsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Way to check that the dictionary doesn't contain duplicated values (source: https://www.geeksforgeeks.org/python-test-if-dictionary-contains-unique-keys-and-values/)\n",
    "# # using loops\n",
    "# # check for unique values\n",
    "# flag = False\n",
    "# hash_val = dict()\n",
    "# for keys in senders:\n",
    "#     if senders[keys] in hash_val:\n",
    "#         flag = True\n",
    "#         break\n",
    "#     else :\n",
    "#         hash_val[senders[keys]] = 1\n",
    "  \n",
    "# # print result\n",
    "# print(\"Does dictionary contain repetition : \" + str(flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values and keys\n",
    "key_list = list(personsDict.keys())\n",
    "value_list = list(personsDict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e22a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return key for any value\n",
    "def get_key(val):\n",
    "    for key, value in personsDict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test: find Id per value using list comprehension\n",
    "# dict_items = personsDict.items()\n",
    "# myValue = \"Johannes Georgius Graevius^1632^1703^0\"\n",
    "# myKey = [key for key,value in dict_items if value==myValue]\n",
    "# print(myKey)\n",
    "\n",
    "# #_Unidentified_017690^0^0^1687\n",
    "# #Jacobus Gronovius^1645^1716^0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c76a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dict_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b6081",
   "metadata": {},
   "source": [
    "## Create correspondent pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87726c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy\n",
    "combinedSet_v00 = combinedSetGarfCEN.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33349d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dca17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in combinedSet_v00.iterrows():\n",
    "    #capture personId for sender\n",
    "    senderId = combinedSet_v00.iloc[index,1]\n",
    "    #capture personId for receiver\n",
    "    receiverId = combinedSet_v00.iloc[index,3]\n",
    "    #capture personStrId for sender\n",
    "    senderName = combinedSet_v00.iloc[index,2]\n",
    "    #capture personStrId for receiver\n",
    "    receiverName = combinedSet_v00.iloc[index,4]\n",
    "    #find the first one in alphabetical order\n",
    "    firstPairName = min(senderName,receiverName)\n",
    "    # Get the key (personId) for this pair name using the dictionary created in the previous step\n",
    "    IdFirstName = get_key(firstPairName)\n",
    "    #find the second one in alphabetical order\n",
    "    secondPairName = max(senderName,receiverName)\n",
    "    # Get the key (personId) for this pair name using the dictionary created in the previous step\n",
    "    IdSecondName = get_key(secondPairName)\n",
    "    #add the names to the dataframe\n",
    "    combinedSet_v00.loc[index,'correspPairNames'] = firstPairName + \"@\" + secondPairName\n",
    "    combinedSet_v00.loc[index,'correspPairIds'] = IdFirstName + \"@\" + IdSecondName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v00.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd36a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v00.correspPairNames.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v00.correspPairIds.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b367294",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy -> resulting set with correspondent pairs (names) in alphabetical order\n",
    "combinedSet_v01 = combinedSet_v00.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349034b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test, here add Id of a letter I know has multiple senders/receivers\n",
    "# test06 = combinedSet_v01[combinedSet_v01.SKletterId.str.contains('letOcr_011612')]\n",
    "# #skp025230\n",
    "\n",
    "# test06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test06.correspPairNames.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v01.SKletterId.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a5a61",
   "metadata": {},
   "source": [
    "## Create letter year ranges for each pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be517e",
   "metadata": {},
   "source": [
    "Each pair of correspondents may have exchanged letters for a period of years. To create a range of letter exchanges we use the earliest year and the latest year of the letters that a pair exchanged as the range borders. However, there is an issue with \"UNCERTAINTY\" in the letter dates. With the aim to generate the minimun and maximum years to create the time range in which a pair of correspondents exchanged letters, also keeping into account the degree of uncertainty, we have divided the dataset into two groups:\n",
    "\n",
    "1. slice1_certain: We group here:\n",
    "- the letters that have specific years and these years are certain.\n",
    "- the original letter record (only in the CEN dataset) was used to describe not only one letter but several letters. E.g., one row is to describe 60 letters, which were written between 1688 and 1680. In this case, the range doesn't indicate uncertainty, but an actual period of time in which several letters were exchanged.\n",
    "\n",
    "2. slice2_uncertain: The other group is made up of letters which letter year falls into any of the \"uncertainty\" cases below. We make a separate group with these letters because they have the potential of introducing noise in the creation of the ranges in which two persons were actively exchanging letters. For this group we use a standard year range, which is assigned based on the years of birth/death of the correspondents. When these don't exist, we use the years in which Magliabechi was active as correspondent (1650-1714).\n",
    "- A letter may have a specific year, but this is uncertain (as indicated by a question mark or any other uncertainty mark in the original metadata, e.g., 1520?).\n",
    "- It can also happen that uncertainty was indicated by adding a range (e.g., a letter was written in some year between 1688 and 1680). \n",
    "- Several letters described with a range (as above) may also be uncertain or incorrect. For example, in this CEN record (https://picarta.oclc.org/psi/DB=3.23/XMLPRS=Y/PPN?PPN=310885922) the range of letter exchange indicated by the cataloguer was 1641-1674 (for 3 letters). However, the dates of birth and death of the correspondents let see that there is a mistake in that range, since one of the correspondents was already dead in the ending year of the range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc2f71",
   "metadata": {},
   "source": [
    "### For pairs with letter years in slice1_certain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ecb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy\n",
    "combinedSet_v02 = combinedSet_v01.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e61cb0",
   "metadata": {},
   "source": [
    "#### Create subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask for letters which fullfil the requirements of Group1 as described above\n",
    "# first criterium: letter has specific years (no ranges) --> isYearRange == 'n' and the specific year of the letter is not uncertain --> isYearItemUncertain == 'n'\n",
    "# second criterium: letter has a range --> isYearRange == 'n' and this range is not uncertain --> isYearRangeUncertain == 'n'\n",
    "\n",
    "combinedSet_v03 = combinedSet_v02[((combinedSet_v02.isYearRange == 'n') & (combinedSet_v02.isYearItemUncertain == 'n')) | ((combinedSet_v02.isYearRange == 'y') & (combinedSet_v02.isYearRangeUncertain == 'n'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce5c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v03.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a09c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy\n",
    "combinedSet_v04 = combinedSet_v03.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86228c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "letterscount_slice1 = combinedSet_v04.SKletterId.nunique()\n",
    "letterscount_slice1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedSet_v04.yearItem_end.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedSet_v04.yearItem_end.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking\n",
    "combinedSet_v04[combinedSet_v04.correspPairIds.str.contains('skp024024@skp028586')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c4632",
   "metadata": {},
   "source": [
    "#### Get the correspondence year ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the ranges of years (minimum and maximum years in which a pair exchanged correspondence, we fill in Year_end with Year_start in the case the Year_end was not present.\n",
    "# This is to get the maximum year of correspondence from the second column.\n",
    "\n",
    "# first create a temporary copy to work on the \"yearItem_end\"\n",
    "combinedSet_v04['yearItem_end_temp'] = combinedSet_v04['yearItem_end']\n",
    "\n",
    "# now replace the '0' for NaN\n",
    "combinedSet_v04['yearItem_end_temp'].replace(0, np.nan, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedSet_v04.yearItem_end_temp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy\n",
    "combinedSet_v05 = combinedSet_v04.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v05.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739118bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedSet_v05.yearItem_end_temp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in blanks in yearEnd with yearStart and change datatype\n",
    "combinedSet_v05['yearItem_end_temp'] = combinedSet_v05['yearItem_end_temp'].fillna(combinedSet_v05['yearItem_start'])\n",
    "combinedSet_v05['yearItem_end_temp'] = combinedSet_v05['yearItem_end_temp'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990281fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v06 = combinedSet_v05.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v06.yearItem_end.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ee641",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v06.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04708280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedSet_v06.yearItem_end_temp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedSet_v06.yearItem_end_temp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c58a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group per correspPairs getting the min year per pair\n",
    "combinedSet_v07 = combinedSet_v06.groupby('correspPairIds')['yearItem_start'].min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef354dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedSet_v07.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test07 = combinedSet_v07[combinedSet_v07.correspPairIds.str.contains('skp024024@skp028586')]\n",
    "# #letOcr_011612\n",
    "# #letOcr_006148\n",
    "# #'_Unidentified_017690'\n",
    "# #skp046382\n",
    "\n",
    "\n",
    "test07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8bc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group per correspPairs getting the max year per pair. Here I use the column I created with the values from Year1\n",
    "# in Year2, because the higher year is part of a range (be in column Year2) or be part of Year1\n",
    "combinedSet_v08 = combinedSet_v06.groupby('correspPairIds')['yearItem_end_temp'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v08.tail(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d393cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test08 = combinedSet_v08[combinedSet_v08.correspPairIds.str.contains('skp024024@skp028586')]\n",
    "#letOcr_011612\n",
    "#'_Unidentified_017690'\n",
    "#skp046382\n",
    "test08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19263b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes with min and max values\n",
    "combinedSet_v09 = combinedSet_v07.merge(combinedSet_v08, how = 'inner', left_on = 'correspPairIds', right_on = 'correspPairIds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v09.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "combinedSet_v09.rename(columns={\"yearItem_start\":\"correspYearStart\", \"yearItem_end_temp\":\"correspYearEnd\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test09 = combinedSet_v09[combinedSet_v09.correspPairIds.str.contains('skp024024@skp028586')] #gronovius/graevius\n",
    "#_Unidentified_017690\n",
    "#letOcr_011612\n",
    "#skp016830 #Cuper?\n",
    "\n",
    "test09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f988cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v09.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e097a9e",
   "metadata": {},
   "source": [
    "#### Add these years to letters file from this group\n",
    "Here I use the letters file in this group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf5d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add these years to the df with all letters in this set\n",
    "combinedSet_v10 = combinedSet_v09.merge(combinedSet_v04, how = 'outer', left_on = 'correspPairIds', right_on = 'correspPairIds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a value in a new column indicating the level of certainty of this range\n",
    "combinedSet_v10['_tempTypePair'] = 'slice1_certain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795eca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v11 = combinedSet_v10.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ad6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v11.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbf3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# test10 = combinedSet_v11[combinedSet_v11.SKletterId.str.contains('skL00008845')]\n",
    "# #skL00008845\n",
    "# #letOcr_011612\n",
    "# #letOcr_006148\n",
    "\n",
    "# test10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# # test11 = combinedSet_v11[combinedSet_v11.correspPairIds.str.contains('mop0670')]\n",
    "# test11 = combinedSet_v11[combinedSet_v11.SKletterId.str.contains('skL00056111')]\n",
    "# #skL00056111\n",
    "# #letOcr_011612\n",
    "\n",
    "# test11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3027286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test11.iloc[0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f4dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# certainYears = combinedSet_v11[combinedSet_v11.isYearItemUncertain.str.contains('n')]\n",
    "# certainYears.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28bd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep necessary columns only. Since from here onwards we only work with correspondent pairs and the time ranges of their correspondence (ignoring the individual letters), we drop unnecessary columns\n",
    "combinedSet_v12 = combinedSet_v11[['correspPairIds', 'correspPairNames', 'SKpersonId_sender', 'personStrId_sender', 'SKpersonId_receiver', 'personStrId_receiver', 'correspYearStart', 'correspYearEnd', '_tempTypePair']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v12.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f39c37",
   "metadata": {},
   "source": [
    "### For pairs with letter years in slice2_uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2369609",
   "metadata": {},
   "source": [
    "#### Create subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb07509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask for letters which fullfil the requirements of slice2_uncertain as described above\n",
    "# first criterium: a letter has a specific year but this is uncertain\n",
    "# second criterium: when the year was added as a range to indicate uncertainty -->  isYearRangeUncertain == 'y' & isYearItemUncertain == 'y' (this combination captures the years which are uncertain and estimated via a range)\n",
    "# third criterium: several letters included in a range have an incorrect or uncertain range --> isYearRangeUncertain == 'y' & isYearItemUncertain == 'n.a.' (this means that the range was not used to estimate a year, but it was itself uncertain)\n",
    "\n",
    "combinedSet_v14 = combinedSet_v02[((combinedSet_v02.isYearRange == 'n') & (combinedSet_v02.isYearItemUncertain == 'y')) | ((combinedSet_v02.isYearRangeUncertain == 'y') & (combinedSet_v02.isYearItemUncertain == 'y')) | ((combinedSet_v02.isYearRangeUncertain == 'y') & (combinedSet_v02.isYearItemUncertain == 'n.a.'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v14.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd40f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy to reset index\n",
    "combinedSet_v15 = combinedSet_v14.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "letterscount_slice2 = combinedSet_v15.SKletterId.nunique()\n",
    "letterscount_slice2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c426dbb",
   "metadata": {},
   "source": [
    "#### Split dates of birth/death/fl of correspondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dates of birth/death/fl. of correspondents (senders)\n",
    "combinedSet_v16 = combinedSet_v15['personStrId_sender'].str.split(\"^\", n = 4, expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2548ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add those fields to the main df (senders)\n",
    "combinedSet_v15['nameString_sender'] = combinedSet_v16[0].astype(str)\n",
    "combinedSet_v15['dateBirth_sender'] = combinedSet_v16[1].astype(int)\n",
    "combinedSet_v15['dateDeath_sender'] = combinedSet_v16[2].astype(int)\n",
    "combinedSet_v15['dateFl_sender'] = combinedSet_v16[3].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7422a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy\n",
    "combinedSet_v17 = combinedSet_v15.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f880d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v17.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860eede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dates of birth/death/fl. of correspondents (receivers)\n",
    "combinedSet_v18 = combinedSet_v15['personStrId_receiver'].str.split(\"^\", n = 4, expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be39620",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17346b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add those fields to the main df (receivers)\n",
    "combinedSet_v17['nameString_receiver'] = combinedSet_v18[0].astype(str)\n",
    "combinedSet_v17['dateBirth_receiver'] = combinedSet_v18[1].astype(int)\n",
    "combinedSet_v17['dateDeath_receiver'] = combinedSet_v18[2].astype(int)\n",
    "combinedSet_v17['dateFl_receiver'] = combinedSet_v18[3].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52342ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v17.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy\n",
    "combinedSet_v19 = combinedSet_v17.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d145a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v19.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8b2f0",
   "metadata": {},
   "source": [
    "#### Assign min and max year of correspondence exchange using dates of birth/death/fl of correspondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68011eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if sender and receiver have both db and dd, select: latest date of birth and earliest date of death\n",
    "\n",
    "for index, row in combinedSet_v19.iterrows():\n",
    "    #check if both sender and receiver have db and dd\n",
    "    if (row['dateBirth_sender'] != 0 and row['dateDeath_sender'] != 0) and (row['dateBirth_receiver'] != 0 and row['dateDeath_receiver'] != 0):\n",
    "        #check which is the latest date of birth and add 15 years\n",
    "        if row['dateBirth_sender'] <= row['dateBirth_receiver']:\n",
    "            #capture value\n",
    "            latestDb = row['dateBirth_receiver'] + 15 #ToDo: find the lowest date of birth in this set and use that as value\n",
    "            #add latest date of birth + 15 to column\n",
    "            combinedSet_v19.at[index,'correspYearStart'] = latestDb\n",
    "        elif row['dateBirth_receiver'] <= row['dateBirth_sender']:\n",
    "            #capture value\n",
    "            latestDb = row['dateBirth_sender'] + 15 #ToDo: find the lowest date of birth in this set and use that as value\n",
    "            #add latest date of birth + 15 to column\n",
    "            combinedSet_v19.at[index,'correspYearStart'] = latestDb\n",
    "        #check which is the earliest date of death\n",
    "        if row['dateDeath_sender'] <= row['dateDeath_receiver']:\n",
    "            #add earliest date of death to column\n",
    "            combinedSet_v19.at[index,'correspYearEnd'] = row['dateDeath_sender']\n",
    "        elif row['dateDeath_receiver'] <= row['dateDeath_sender']:\n",
    "            #add earliest date of death to column\n",
    "            combinedSet_v19.at[index,'correspYearEnd'] = row['dateDeath_receiver']\n",
    "    # if there were no dates, use the default range in which Magliabechi was actively corresponding\n",
    "    else:\n",
    "        combinedSet_v19.at[index,'correspYearStart'] = 1650\n",
    "        combinedSet_v19.at[index,'correspYearEnd'] = 1714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f52e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v19.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51775fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v19.correspYearEnd.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320fd843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes\n",
    "combinedSet_v19['correspYearStart'] = combinedSet_v19['correspYearStart'].astype(int)\n",
    "combinedSet_v19['correspYearEnd'] = combinedSet_v19['correspYearEnd'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a value in a new column indicating the level of certainty of this range\n",
    "combinedSet_v19['_tempTypePair'] = 'slice2_uncertain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v19.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ed250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## remember that there are 72 letters from Garfagnini which are uncertain, and 549 from CEN\n",
    "\n",
    "# cenUncertain = combinedSet_v20[combinedSet_v20.SKletterId.str.contains('skL')]\n",
    "# cenUncertain.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadca286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep necessary columns only. Since from here onwards we only work with correspondent pairs and the time ranges of their correspondence (ignoring the individual letters), we drop unnecessary columns\n",
    "combinedSet_v20 = combinedSet_v19[['correspPairIds', 'correspPairNames', 'SKpersonId_sender', 'personStrId_sender', 'SKpersonId_receiver', 'personStrId_receiver', 'correspYearStart', 'correspYearEnd', '_tempTypePair']] ##_tempTypePair: this column indicates level of certainty in the year ranges (two groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# test14 = combinedSet_v20[combinedSet_v20.SKletterId.str.contains('skL00001097')]\n",
    "\n",
    "# test14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v21 = combinedSet_v20.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v21.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7c9f9",
   "metadata": {},
   "source": [
    "# Prepare final unique correspondent pairs dataset\n",
    "\n",
    "The previous two datasets are slices of the entire aggregated letter set of CEN and Garfagnini (up to 2nd degree of Magliabechi). One slice is created using only letter year values which were certain. While the other slice was created using the uncertain year values. Even though in our research we want to use the most certain values, we also don't want to loose the data of existing relationships between two persons. For example, we can have a correspondence pair a-b in the \"certain\" slice, and also a pair a-b in the \"uncertain\" slice. In that case, we prefere the data (i.e., year ranges) about a-b from the certain slice. But, if there was a relationship d-e in the \"uncertain\" dataset with no equivalent d-e values in the \"certain\" dataset, we want to include d-e in our final dataset, even though the ranges of letter exchange are uncertain. We keep of course an uncertainty mark to be able to filter in or out those pairs during our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd93da7",
   "metadata": {},
   "source": [
    "## Combine slice2_uncertain with slice1_certain\n",
    "In this step, we check if the same pair combination in the \"uncertain\" slice exists or not in the \"certain\" slice. If it does, we discard it, if it doesn't, we keep it in our resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe167489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now put together the previous groups using the rules mentioned above\n",
    "# combinedSet_v12 is the \"certain\" ranges dataset, combinedSet_v21 is the \"uncertain\" ranges dataset\n",
    "# First obtain the rows from the uncertain dataset which are NOT in the certain dataset (using the correspPairIds)\n",
    "df_diff = combinedSet_v21[~combinedSet_v21.correspPairIds.isin(combinedSet_v12.correspPairIds)]\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/64582124/appending-only-rows-that-are-not-yet-in-a-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca196113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff.correspPairIds.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0165049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine the pairs with the certain slice with the pairs of the uncertain slice which are not in the certain slice (df_diff)\n",
    "combinedSet_v30 = pd.concat([combinedSet_v12, df_diff], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v30.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15491d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSet_v30.correspPairIds.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b525f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# test90 = combinedSet_v30[combinedSet_v30.correspPairIds.str.contains('skp024024@skp028586')]\n",
    "# #_Unidentified_017690\n",
    "# #letOcr_011612\n",
    "\n",
    "# test90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92399e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see = test90.iloc[1,10]\n",
    "# see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGarfCENGroups = combinedSet_v30.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b098a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGarfCENGroups.correspPairIds.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d80c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGarfCENGroups._tempTypePair.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11eece",
   "metadata": {},
   "source": [
    "## Drop duplicates\n",
    "In this step, duplicated groups are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3623bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only one row with correspPairIds (drop duplicates using subset parameter, to make sure that if persons \n",
    "# are in inverse order they also get dropped (have to drop based on correspGroupId\n",
    "combinedSetGroups_v01 = combinedSetGarfCENGroups.drop_duplicates(subset=['correspPairIds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64af578",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroups_v01.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroups_v02 = combinedSetGroups_v01.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroups_v02.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# test15 = combinedSetGroups_v02[combinedSetGroups_v02.correspPairNames.str.contains('Rijckius')]\n",
    "# # skp003623 'Magliabechi\n",
    "# #_Unidentified_017690\n",
    "# #letOcr_011612\n",
    "\n",
    "# test15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test15._tempTypePair.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6bdde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# test16 = combinedSetGroups_v02[combinedSetGroups_v02.correspPairNames.str.contains('Bellini Lorenzo')] \n",
    "# #_Unidentified_017690\n",
    "# #letOcr_011612\n",
    "\n",
    "# test16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the resulting set that contains only the group names + Ids plus the ranges\n",
    "combinedSetGroups = combinedSetGroups_v02.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd93385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check groups\n",
    "# test17 = combinedSetGroups[combinedSetGroups.correspPairIds.str.contains('skp024024@skp042335')] \n",
    "# test17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21976e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroups._tempTypePair.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f81bf",
   "metadata": {},
   "source": [
    "## Split dates of birth/date/fl of correspondents \n",
    "to be used in the forthcoming cases = ranges of positive/negative ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy\n",
    "combinedSetGroupsPersonDates_v00 = combinedSetGroups.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d48490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dates of birth/death/fl. of correspondents (senders)\n",
    "combinedSetGroupsPersonDates_v01 = combinedSetGroupsPersonDates_v00['personStrId_sender'].str.split(\"^\", n = 4, expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsPersonDates_v01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c874352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add those fields to the main df (senders)\n",
    "combinedSetGroupsPersonDates_v00['nameString_sender'] = combinedSetGroupsPersonDates_v01[0].astype(str)\n",
    "combinedSetGroupsPersonDates_v00['dateBirth_sender'] = combinedSetGroupsPersonDates_v01[1].astype(int)\n",
    "combinedSetGroupsPersonDates_v00['dateDeath_sender'] = combinedSetGroupsPersonDates_v01[2].astype(int)\n",
    "combinedSetGroupsPersonDates_v00['dateFl_sender'] = combinedSetGroupsPersonDates_v01[3].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754edc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy\n",
    "combinedSetGroupsPersonDates_v02 = combinedSetGroupsPersonDates_v01.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b78f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsPersonDates_v02.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca244f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dates of birth/death/fl. of correspondents (receivers)\n",
    "combinedSetGroupsPersonDates_v03 = combinedSetGroupsPersonDates_v00['personStrId_receiver'].str.split(\"^\", n = 4, expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsPersonDates_v03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add those fields to the main df (receivers)\n",
    "combinedSetGroupsPersonDates_v00['nameString_receiver'] = combinedSetGroupsPersonDates_v03[0].astype(str)\n",
    "combinedSetGroupsPersonDates_v00['dateBirth_receiver'] = combinedSetGroupsPersonDates_v03[1].astype(int)\n",
    "combinedSetGroupsPersonDates_v00['dateDeath_receiver'] = combinedSetGroupsPersonDates_v03[2].astype(int)\n",
    "combinedSetGroupsPersonDates_v00['dateFl_receiver'] = combinedSetGroupsPersonDates_v03[3].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6316320",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsPersonDates_v00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97972a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy\n",
    "combinedSetGroupsPersonDates_v04 = combinedSetGroupsPersonDates_v00.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c4c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsPersonDates_v04.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsPersonDates_v04.dateDeath_sender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a5c5e9",
   "metadata": {},
   "source": [
    "## Find earliest date of death of the correspondents (pending to complete)\n",
    "This is necessary for the next step, to determine the boundaries of the positive or negative relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in combinedSetGroupsPersonDates_v04.iterrows():\n",
    "    # determine which is the earliest date of death when both sender and receiver have dateBirth and dateDeath\n",
    "    # check if both sender and receiver have dateDeath\n",
    "    if (row['dateDeath_sender'] != 0) & (row['dateDeath_receiver'] != 0):\n",
    "        # check which is the earliest date of death\n",
    "        if row['dateDeath_sender'] <= row['dateDeath_receiver']:\n",
    "            #add earliest date of death to a column\n",
    "            combinedSetGroupsPersonDates_v04.at[index,'dateDeath_earliest'] = row['dateDeath_sender']\n",
    "        # check which is the earliest date of death\n",
    "        elif row['dateDeath_receiver'] <= row['dateDeath_sender']:\n",
    "            #add earliest date of death to a column\n",
    "            combinedSetGroupsPersonDates_v04.at[index,'dateDeath_earliest'] = row['dateDeath_receiver']\n",
    "    # determine which is the earliest date of death when only sender has dateDeath\n",
    "    # check if sender has dateDeath and receiver has not\n",
    "    if (row['dateDeath_sender'] != 0) & (row['dateDeath_receiver'] == 0):\n",
    "        # sender's dateDeath is the earliest date of death\n",
    "        #add earliest date of death to a column\n",
    "        combinedSetGroupsPersonDates_v04.at[index,'dateDeath_earliest'] = row['dateDeath_sender']\n",
    "    # determine which is the earliest date of death when only receiver has dateDeath\n",
    "    # check if receiver has dateDeath and sender has not\n",
    "    if (row['dateDeath_sender'] == 0) & (row['dateDeath_receiver'] != 0):\n",
    "        # receiver's dateDeath is the earliest date of death\n",
    "        #add earliest date of death to a column\n",
    "        combinedSetGroupsPersonDates_v04.at[index,'dateDeath_earliest'] = row['dateDeath_receiver']\n",
    "    # if nor sender/receiver have dateDeath add Magliabechi's dateDeath as earliest date of death\n",
    "    elif (row['dateDeath_sender'] == 0) & (row['dateDeath_receiver'] == 0):\n",
    "        combinedSetGroupsPersonDates_v04.at[index,'dateDeath_earliest'] = 1714\n",
    "\n",
    "        \n",
    "# ##\n",
    "# for index, row in combinedSetGroupsPersonDates_v04.iterrows():\n",
    "#     # determine which is the earliest date of death\n",
    "#     if (row['dateBirth_sender'] != 0 and row['dateDeath_sender'] != 0) and (row['dateBirth_receiver'] != 0 and row['dateDeath_receiver'] != 0):\n",
    "#         # check which is the earliest date of death\n",
    "#         if row['dateDeath_sender'] <= row['dateDeath_receiver']:\n",
    "#             #add earliest date of death to a column\n",
    "#             combinedSetGroupsPersonDates_v04.at[index,'dateDeath_earliest'] = row['dateDeath_sender']\n",
    "#         # check which is the earliest date of death\n",
    "#         elif row['dateDeath_receiver'] <= row['dateDeath_sender']:\n",
    "#             #add earliest date of death to a column\n",
    "#             combinedSetGroupsPersonDates_v04.at[index,'dateDeath_earliest'] = row['dateDeath_receiver']\n",
    "#     # if persons don't have complete dates of birth/death, add date of death of Magliabechi as end of negative relation\n",
    "#     elif (row['dateBirth_sender'] == 0 or row['dateDeath_sender'] == 0 or row['dateBirth_receiver'] == 0 or row['dateDeath_receiver'] == 0):\n",
    "#         combinedSetGroupsPersonDates_v04.at[index,'dateDeath_earliest'] = 1714\n",
    "##\n",
    "\n",
    "## Comments from Ingeborg April 20:\n",
    "# we will use dates of Magliabechi, because even if they are second degree and have no dates of birth and death, it means they\n",
    "# are not so important (because floriat dates were added based on correspondence). TEST this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsPersonDates = combinedSetGroupsPersonDates_v04.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f1230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "\n",
    "combinedSetGroupsPersonDates_columns = combinedSetGroupsPersonDates.columns\n",
    "\n",
    "for column in combinedSetGroupsPersonDates_columns:\n",
    "    dataType = combinedSetGroupsPersonDates.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        combinedSetGroupsPersonDates[column] = combinedSetGroupsPersonDates[column].fillna(0.0)\n",
    "        combinedSetGroupsPersonDates[column] = combinedSetGroupsPersonDates[column].astype(int)\n",
    "    if dataType == object:\n",
    "        combinedSetGroupsPersonDates[column] = combinedSetGroupsPersonDates[column].fillna('null')\n",
    "        combinedSetGroupsPersonDates[column] = combinedSetGroupsPersonDates[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c53ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsPersonDates.dateDeath_earliest.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94866def",
   "metadata": {},
   "source": [
    "## Change column names\n",
    "Each pair of correspondents has now a range of years in which they exchanged correspondence. Here we change the column names and create the final copy of the dataset which contains the unique groups (i.e., pairs of correspondents) with the ranges in which they exchanged letters, using the year letters from the combined datasets that were merged above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6149f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsTies_v00 = combinedSetGroupsPersonDates.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f67b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsTies_v00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ba560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names: since the direction of the relationship is not anymore based on \"sender\"/\"receiver\"\n",
    "combinedSetGroupsTies_v01 = combinedSetGroupsTies_v00.rename(columns={'SKpersonId_sender':'SKpersonId_person1',\n",
    "                                                                      'SKpersonId_receiver':'SKpersonId_person2',\n",
    "                                                                      'personStrId_sender':'personStrId_person1',\n",
    "                                                                      'personStrId_receiver':'personStrId_person2',\n",
    "                                                                      'dateBirth_sender':'dateBirth_person1',\n",
    "                                                                      'dateDeath_sender':'dateDeath_person1', \n",
    "                                                                      'dateFl_sender':'dateFl_person1',\n",
    "                                                                      'dateBirth_receiver':'dateBirth_person2', \n",
    "                                                                      'dateDeath_receiver':'dateDeath_person2', \n",
    "                                                                      'dateFl_receiver':'dateFl_person2'\n",
    "                                                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037fc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert min and max year back to integers\n",
    "combinedSetGroupsTies_v01['correspYearStart'] = combinedSetGroupsTies_v01['correspYearStart'].astype(int)\n",
    "combinedSetGroupsTies_v01['correspYearEnd'] = combinedSetGroupsTies_v01['correspYearEnd'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38639772",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsTies = combinedSetGroupsTies_v01.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8413f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsTies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5cfc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 18\n",
    "# check groups\n",
    "test18 = combinedSetGroupsTies[combinedSetGroupsTies.correspPairIds.str.contains('skp013301')] \n",
    "test18\n",
    "\n",
    "# skp024024@skp028586 (Gronovius/Graevius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "countsCertainty = combinedSetGroupsTies._tempTypePair.value_counts()\n",
    "countsCertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK WITH INGEBORG: influence of uncertain set\n",
    "\n",
    "checkUncertain = combinedSetGroupsTies[combinedSetGroupsTies._tempTypePair.str.contains('slice2_uncertain')] \n",
    "checkUncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ab401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store this file externally to explore\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# fileName15 = (f\"{pathRepository}pairsWithUncertainLetters_{timestr}.csv\")\n",
    "# checkUncertain.to_csv(fileName15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c373fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CHECK WITH INGEBORG: influence of earliest date of death 1714, these are those\n",
    "\n",
    "# checkEarliestDdeath = combinedSetGroupsTies[combinedSetGroupsTies.dateDeath_earliest == 1714] \n",
    "# checkEarliestDdeath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store this file externally to explore\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# fileName5 = (f\"{pathRepository}pairsWithEarliestDateDeath1714_{timestr}.csv\")\n",
    "# checkEarliestDdeath.to_csv(fileName5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d323cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsTies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a86ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsTies.correspPairIds.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f90032",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsTies.correspPairNames.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aff19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedSetGroupsTies.dateDeath_earliest.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e3f51",
   "metadata": {},
   "source": [
    "## Resulting dataset: correspondent pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c19862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store this file externally\n",
    "\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# fileName3 = (f\"{pathFolder}set2_CEN2ndDGarfagnini_groupsAndRanges_{timestr}.csv\")\n",
    "# combinedSetGarfCENGroups.to_csv(fileName3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51c515",
   "metadata": {},
   "source": [
    "# Read and integrate extra data: annotated ties\n",
    "Based on a close reading of a significant sample of Magliabechi's letters, it was discovered that some of them pointed to conflicts between Magliabechi and other scholars. A more detailed description of the collection of negative mentions can be found in Chapter 1 of the R_script_Unlinked_09052022 notebook in this repository. The dating of the negative ties is based on the first negative mention made by Magliabechi in his letters and continues until the death of one of the correspondents unless Magliabechi explicitly mentions a reconciliation. Via manual annotation, a file was created in which each correspondent pair who had a conflict (i.e., negative relationship) was recorded with a negative sign (-1). In addition, pairs that were collaborating together against a common enemy are recorded with a positive sign (1). These was done in two datasets:\n",
    "\n",
    "- One for which we know that the correspondent pairs coincide with pairs in the existing letter dataset (this is datasetA)\n",
    "- One for which we know that there was no correspondent pair coinciding with a pair in the existing letter dataset, these pairs are new (this is datasetB)\n",
    "\n",
    "We have created rules and cases for integrating these datasets. The resulting set is a \"relationship\" dataset with correspondent pairs, year ranges of their relation, and a sign on whether this was positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac94959",
   "metadata": {},
   "source": [
    "## Extra dataA: annotated NEGATIVE ties for previous groups\n",
    "This file contains manually annotated NEGATIVE ties for the groups above, i.e., it is a file that adds negative relations to the existing dataset. For adding extra groups, i.e., correspondent pairs, which are non-existing in the previous dataset, use the other file import: \"Extra dataB\" below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a0dca",
   "metadata": {},
   "source": [
    "### Upload dataset A\n",
    "This file contains manually added negative ties to some of the previous groups. A negative tie is indicated wiht -1. This happens when there is evidence of a conflict or negative relation on a specific year.\n",
    "1. We import dataset A in the required format:\n",
    "- correspPairIds --same as in the latest combined dataset \"combinedSetGroupsTies\" if it exists\n",
    "- Optional columns: SKpersonId_person1; personStrId_person1; SKpersonId_person2; personStrId_person2. These columns are to register the person Ids and their name strings during the creation of the annotated ties, but they are not mandatory for the import. If imported, they will be dropped later.\n",
    "- startYear_Conflict: this is a year in which a conflict started. Register in the format \\d\\d\\d\\d. It is mandatory!\n",
    "- endYear_Conflict: this year is only registered if there is evidence that a conflict ended. If not, leave empty or register a zero, the end value will be filled in automatically (the rules are described later). This column is mandatory!\n",
    "- signRelation: because this file is to register negative ties from the existing dataset, the default annotation is \"-1\" (the sign for a negative relation). This column is mandatory.\n",
    "- notesConflict: add any notes in free text about the negative relation. This column is not mandatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09116f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read annotated negative tiesA file\n",
    "negative_tiesA_path = os.path.join(data_raw_directory, 'extraPairsWithTiesA_20220414_181600.csv')\n",
    "\n",
    "annotatedTiesA_v00 = pd.read_csv(negative_tiesA_path, sep = \",\", index_col=False, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesA_v00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350be6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesA_v00.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f19b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "\n",
    "annotatedTiesA_columns = annotatedTiesA_v00.columns\n",
    "\n",
    "for column in annotatedTiesA_columns:\n",
    "    dataType = annotatedTiesA_v00.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        annotatedTiesA_v00[column] = annotatedTiesA_v00[column].fillna(0.0)\n",
    "        annotatedTiesA_v00[column] = annotatedTiesA_v00[column].astype(int)\n",
    "    if dataType == object:\n",
    "        annotatedTiesA_v00[column] = annotatedTiesA_v00[column].fillna('null')\n",
    "        annotatedTiesA_v00[column] = annotatedTiesA_v00[column].astype(str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fe9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesA_v1 = annotatedTiesA_v00.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesA_v1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e35d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 19\n",
    "# check groups\n",
    "test19 = annotatedTiesA_v1[annotatedTiesA_v1.correspPairIds.str.contains('skp024024@skp028586')] \n",
    "test19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0abcb4",
   "metadata": {},
   "source": [
    "### Integrate datasetA data with existing groups\n",
    "\n",
    "The correspGroupId is used to combine the previous dataset (combinedSetGroupsTies) with the imported annotated negative ties set (dataset A). The purpose is to obtain the ranges for positive and negative relationships. We do it in this way:\n",
    "\n",
    "1. We use iterrows to check if there is a matching correspGroupId between the existing dataset and the annotated datasetA. If there is a match, we adjust the end year of the conflict (conflictYearEnd) depending on different rules and cases:\n",
    "\n",
    "- Rule1: when we know that a conflict started but we don't have evidence on when/whether it ended, the \"conflictYearEnd\" value is converted to 0 (empty) in the annotated datasetA, and we use the earliest date of death of the correspondents as a default year for the end of the conflict. We make this assumption that a conflict only ends when one of the participants dies, unless there is evidence that they reconciliated (Rule2).\n",
    "\n",
    "- Rule2: when we know that a conflict started and also do have evidence that it ended and when, the value for \"conflictYearEnd\" is hard-coded/written in the original annotated datasetA that was imported before. \n",
    "\n",
    "2. After filling in all the values for \"conflictYearEnd\" in the annotated relations file (step1) according to the two rules above, we determine the new ranges for different relations between correspondents based on the conflict year. We assign a \"relationYearStart\" and \"relationYearEnd\" and the type of relation for those ranges (positive or negative) using these cases:\n",
    "\n",
    "- CASES type A: Those that fall into Rule1 above. As default for all the cases of typeA the conflict ends with the earliest year of death of one of the correspondents. We then define the relationships in this way:\n",
    "    * CaseA1. The conflict and the correspondence start at the same time:\n",
    "        - Positive relation: none.\n",
    "        - Negative relation: yes, in which relationYearStart = conflictYearStart = correspYearStart until relationYearEnd = conflictYearEnd = earliest date of death.\n",
    "\n",
    "    * CaseA2. The conflict starts after the correspondence starts: \n",
    "        - Positive relation: yes, in which relationYearStart = correspYearStart until relationYearEnd = conflictYearStart minus 1.\n",
    "        - Negative relation: yes, in which relationYearStart = conflictYearStart until relationYearEnd = earliest date of death of the correspondents.<br />\n",
    "    For example, the relation between Apollonio Bassetti^1631^1699^0 and Antonio Marco Magliabechi^1633^1714^0 includes correspondence exchange between 1667 and 1695. However, there is evidence of a conflict that started in 1674, and there is no evidence that this conflict ended. We then create two ranges: one for the positive relation (1667 until 1673) and another one for the negative relation (1674 until 1699) being 1699 Bassetti's date of death.\n",
    "\n",
    "    * CaseA3. The conflict starts before the correspondence starts. In this case, because we don't know whether the conflict was resolved when the letter exchange started, we create the same relation as in case A1.\n",
    "\n",
    "\n",
    "- CASES type B: Those tlat fall into Rule2 above. In these cases, the date in which a conflict ends is known. Because of this, we create different relations depending on the start and end year of the correspondence. Because these cases are many, we have created a separate table (HERE GOES EXCEL TABLE) with them. Here below we only explain and code the cases that apply to our data.\n",
    "\n",
    "    * CaseB9. If the conflict starts before the start period of a correspondence and ends before the end year of the correspondence: we split this group into two relations:\n",
    "        - Negative relation: in which relationYearStart =  conflictYearStart until the year in which a conflict ended (conflictYearEnd).\n",
    "        - Positive relation: in which relationYearStart = conflictYearEnd plus 1, until the end year of the correspondence.<br />\n",
    "    For example, the relationship between Jacobus Gronovius^1645^1716^0 and Johannes Georgius Graevius^1632^1703^0 is represented by letter exchanges between 1682 and 1692 in the combined letter dataset. There is evidence that there was a conflict in 1681 and that they reconciled in 1682. Thus, we create a negative relation in 1681, and a positive relation between 1682 and 1692 (correspYearEnd). \n",
    "    \n",
    "    \n",
    "checked already, we choose the first one:    \n",
    "(NOTE: Inbegorg please check, I chose the end of correspondence as the boundary because somehow this matches more the logics of the table with cases typeB...) but we could also put the boundaries of the positive relation between 1682 and 1703 (date of death of Johannes Georgius Graevius) and say that we keep the boundary years of the conflict as they were found to be based on close reading, ignoring the years in which letters were exchanged, something like: Even though we have the year in which the corresponded ended, we assume that the correspondents kept a positive relation after the conflict ended since there is no evidence that the correspondence that was written after the conflict ended is friendly or not.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eba221",
   "metadata": {},
   "source": [
    "#### Add end of conflict year to annotated dataset using rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b090d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# iterate rows in the annotated ties file checking if there is a common group Id in the main dataset.\n",
    "# In that case, and if the end year of the conflict is missing, fill in the earliest date of death in the annotatedTiesApersonDates set\n",
    "# Note: dates of birth and death are values present the combinedSetGroupsTies not in the annotatedTiesApersonDates\n",
    "\n",
    "for indexA, rowA in annotatedTiesA_v1.iterrows():\n",
    "    for indexB, rowB in combinedSetGroupsTies.iterrows():\n",
    "        # check which rows have the same groupId\n",
    "        if rowA['correspPairIds'] == rowB['correspPairIds']:\n",
    "            # check rule1 (if the end of the relation is unknown, we use earliest date of death as default and don't generate a positive relation if the correspondence )\n",
    "            if rowA['conflictYearEnd'] == 0:\n",
    "                #add a column with the type of case (casesA: end of conflict not available, )\n",
    "                annotatedTiesA_v1.loc[indexA,'conflictCaseType'] = \"A\"\n",
    "                # add the earliest date of death (step 5.4) as end of the conflict\n",
    "                #add earliest date of death to annotatedTiesApersonDates as end of conflict\n",
    "                annotatedTiesA_v1.at[indexA,'conflictYearEnd'] = rowB['dateDeath_earliest']\n",
    "            # check rule2 (if the end of the relation is known), for now just add the type of case (casesB: end of conflict is known)\n",
    "            elif rowA['conflictYearEnd'] != 0:\n",
    "                #add a column with the type of case (casesA: end of conflict not available, casesB: end of conflict is known)\n",
    "                annotatedTiesA_v1.loc[indexA,'conflictCaseType'] = \"B\"\n",
    "\n",
    "                \n",
    "### version May 3, 2022#####\n",
    "# for indexA, rowA in annotatedTiesA_v1.iterrows():\n",
    "#     for indexB, rowB in combinedSetGroupsTies.iterrows():\n",
    "#         # check which rows have the same groupId\n",
    "#         if rowA['correspPairIds'] == rowB['correspPairIds']:\n",
    "#             # check rule1 (if the end of the relation is unknown, we use earliest date of death as default and don't generate a positive relation if the correspondence )\n",
    "#             if rowA['conflictYearEnd'] == 0:\n",
    "#                 #add a column with the type of case (casesA: end of conflict not available, casesB: end of conflict is known)\n",
    "#                 annotatedTiesA_v1.loc[indexA,'conflictCaseType'] = \"A\"\n",
    "#                 # determine which is the earliest date of death\n",
    "#                 if (rowB['dateBirth_person1'] != 0 and rowB['dateDeath_person1'] != 0) and (rowB['dateBirth_person2'] != 0 and rowB['dateDeath_person2'] != 0):\n",
    "#                     # check which is the earliest date of death\n",
    "#                     if rowB['dateDeath_person1'] <= rowB['dateDeath_person2']:\n",
    "#                         #add earliest date of death to annotatedTiesApersonDates as end of conflict\n",
    "#                         annotatedTiesA_v1.at[indexA,'conflictYearEnd'] = rowB['dateDeath_person1']\n",
    "#                     # check which is the earliest date of death\n",
    "#                     elif rowB['dateDeath_person2'] <= rowB['dateDeath_person1']:\n",
    "#                         #add earliest date of death to annotatedTiesApersonDates as end of conflict\n",
    "#                         annotatedTiesA_v1.at[indexA,'conflictYearEnd'] = rowB['dateDeath_person2']\n",
    "#                 # if persons don't have complete dates of birth/death, add date of death of Magliabechi as end of negative relation\n",
    "#                 elif (rowB['dateBirth_person1'] == 0 or rowB['dateDeath_person1'] == 0 or rowB['dateBirth_person2'] == 0 or rowB['dateDeath_person2'] == 0):\n",
    "#                     ## ToDoL: here change to earliest date of death, see 5.4\n",
    "#                     annotatedTiesA_v1.at[indexA,'conflictYearEnd'] = 1714\n",
    "#             # check rule2 (if the end of the relation is known), for now just add the type of case\n",
    "#             elif rowA['conflictYearEnd'] != 0:\n",
    "#                 #add a column with the type of case (casesA: end of conflict not available, casesB: end of conflict is known)\n",
    "#                 annotatedTiesA_v1.loc[indexA,'conflictCaseType'] = \"B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ae616",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesA_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the version of the annotatedTiesA file with the end of conflict year filled in using rules\n",
    "annotatedTiesA_v2 = annotatedTiesA_v1.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4473f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesA_v2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e963a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesA_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d2754",
   "metadata": {},
   "source": [
    "#### Generate relations dataset depending on the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that the value for end conflict is filled in in the annotatedTiesApersonDates dataset and that we have labelled the two types of cases depending on the conflict ending year, we create the relations depending on the case\n",
    "# for that purpose, we will create new rows for the updated relations, which only need these metadata: correspPairIds, relationYearStart, relationYearEnd, signRelation\n",
    "\n",
    "dfRelations1_v0 = pd.DataFrame()\n",
    "\n",
    "for indexA, rowA in annotatedTiesA_v2.iterrows():\n",
    "    for indexB, rowB in combinedSetGroupsTies.iterrows():\n",
    "        # check which rows have the same groupId\n",
    "        if rowA['correspPairIds'] == rowB['correspPairIds']:\n",
    "            # first work with cases of type A (in which the end year of the conflict is the same as the earliest date of death)\n",
    "            if rowA['conflictCaseType'] == 'A':\n",
    "                # create relations for cases A1 and A3 (in which the conflict starts earlier or at the same time that the correspondence starts)\n",
    "                if (rowA['conflictYearStart'] <= rowB['correspYearStart']):\n",
    "                    # update relations in combinedSetGroupsTies\n",
    "                    # create negative relation in which relationYearStart = conflictYearStart = correspYearStart until relationYearEnd = conflictYearEnd = earliest date of death\n",
    "                    correspPairIds = rowB['correspPairIds']\n",
    "                    SKpersonId_person1 = rowB['SKpersonId_person1']\n",
    "                    personStrId_person1 = rowB['personStrId_person1']\n",
    "                    SKpersonId_person2 = rowB['SKpersonId_person2']\n",
    "                    personStrId_person2 = rowB['personStrId_person2']\n",
    "                    relationYearStart = rowA['conflictYearStart']\n",
    "                    relationYearEnd = rowA['conflictYearEnd']\n",
    "                    signRelation = -1\n",
    "                    _tempTypePair = 'slice1_certain'\n",
    "                    notesConflict = rowA['notesConflict']\n",
    "                    dfRelations1_v0 = dfRelations1_v0.append({\n",
    "                        'SKpersonId_person1':SKpersonId_person1,\n",
    "                        'personStrId_person1':personStrId_person1,\n",
    "                        'SKpersonId_person2':SKpersonId_person2,\n",
    "                        'personStrId_person2':personStrId_person2,\n",
    "                        'correspPairIds':correspPairIds,\n",
    "                        'relationYearStart':relationYearStart,\n",
    "                        'relationYearEnd':relationYearEnd,\n",
    "                        'signRelation':signRelation,\n",
    "                        '_tempTypePair':_tempTypePair,\n",
    "                        'notesConflict':notesConflict\n",
    "                    },ignore_index=True)\n",
    "                # create relations for cases A2 (in which the conflict starts after the correspondence starts)\n",
    "                elif (rowB['correspYearStart'] < rowA['conflictYearStart']):\n",
    "                    # update relations in combinedSetGroupsTies\n",
    "                    # create positive relation in which relationYearStart = correspYearStart until relationYearEnd = conflictYearEnd\n",
    "                    correspPairIds = rowB['correspPairIds']\n",
    "                    SKpersonId_person1 = rowB['SKpersonId_person1']\n",
    "                    personStrId_person1 = rowB['personStrId_person1']\n",
    "                    SKpersonId_person2 = rowB['SKpersonId_person2']\n",
    "                    personStrId_person2 = rowB['personStrId_person2']\n",
    "                    relationYearStart = rowB['correspYearStart']\n",
    "                    relationYearEnd = rowA['conflictYearStart']\n",
    "                    signRelation = 1\n",
    "                    _tempTypePair = 'slice1_certain'\n",
    "                    notesConflict = rowA['notesConflict']\n",
    "                    dfRelations1_v0 = dfRelations1_v0.append({\n",
    "                        'SKpersonId_person1':SKpersonId_person1,\n",
    "                        'personStrId_person1':personStrId_person1,\n",
    "                        'SKpersonId_person2':SKpersonId_person2,\n",
    "                        'personStrId_person2':personStrId_person2,\n",
    "                        'correspPairIds':correspPairIds,\n",
    "                        'relationYearStart':relationYearStart,\n",
    "                        'relationYearEnd':relationYearEnd,\n",
    "                        'signRelation':signRelation,\n",
    "                        '_tempTypePair':_tempTypePair,\n",
    "                        'notesConflict':notesConflict\n",
    "                    },ignore_index=True)\n",
    "                    # create negative relation in which relationYearStart = conflictYearStart until relationYearEnd = conflictYearEnd = earliest date of death\n",
    "                    correspPairIds = rowB['correspPairIds']\n",
    "                    SKpersonId_person1 = rowB['SKpersonId_person1']\n",
    "                    personStrId_person1 = rowB['personStrId_person1']\n",
    "                    SKpersonId_person2 = rowB['SKpersonId_person2']\n",
    "                    personStrId_person2 = rowB['personStrId_person2']\n",
    "                    relationYearStart = rowA['conflictYearStart']\n",
    "                    relationYearEnd = rowA['conflictYearEnd']\n",
    "                    signRelation = -1\n",
    "                    _tempTypePair = 'slice1_certain'\n",
    "                    notesConflict = rowA['notesConflict']\n",
    "                    dfRelations1_v0 = dfRelations1_v0.append({\n",
    "                        'SKpersonId_person1':SKpersonId_person1,\n",
    "                        'personStrId_person1':personStrId_person1,\n",
    "                        'SKpersonId_person2':SKpersonId_person2,\n",
    "                        'personStrId_person2':personStrId_person2,\n",
    "                        'correspPairIds':correspPairIds,\n",
    "                        'relationYearStart':relationYearStart,\n",
    "                        'relationYearEnd':relationYearEnd,\n",
    "                        'signRelation':signRelation,\n",
    "                        '_tempTypePair':_tempTypePair,\n",
    "                        'notesConflict':notesConflict\n",
    "                    },ignore_index=True)\n",
    "            # cases typeB are many, here we only code one that is present in our data TODO: code them all?\n",
    "            elif rowA['conflictCaseType'] == 'B':\n",
    "                # create relations for cases B9 (in which the conflict starts before the start period of a correspondence and ends before the end year of the correspondence)\n",
    "                if (rowA['conflictYearStart'] < rowB['correspYearStart']) and (rowA['conflictYearEnd'] < rowB['correspYearEnd']):\n",
    "                    # update relations in combinedSetGroupsTies\n",
    "                    # create negative relation in which in which relationYearStart =  conflictYearStart until the year in which a conflict ended (conflictYearEnd) minus 1 (@Ingeborg ToDo: please check this -1)\n",
    "                    correspPairIds = rowB['correspPairIds']\n",
    "                    SKpersonId_person1 = rowB['SKpersonId_person1']\n",
    "                    personStrId_person1 = rowB['personStrId_person1']\n",
    "                    SKpersonId_person2 = rowB['SKpersonId_person2']\n",
    "                    personStrId_person2 = rowB['personStrId_person2']\n",
    "                    relationYearStart = rowA['conflictYearStart']\n",
    "                    relationYearEnd = rowA['conflictYearEnd']\n",
    "                    signRelation = -1\n",
    "                    _tempTypePair = 'slice1_certain'\n",
    "                    notesConflict = rowA['notesConflict']\n",
    "                    dfRelations1_v0 = dfRelations1_v0.append({\n",
    "                        'SKpersonId_person1':SKpersonId_person1,\n",
    "                        'personStrId_person1':personStrId_person1,\n",
    "                        'SKpersonId_person2':SKpersonId_person2,\n",
    "                        'personStrId_person2':personStrId_person2,\n",
    "                        'correspPairIds':correspPairIds,\n",
    "                        'relationYearStart':relationYearStart,\n",
    "                        'relationYearEnd':relationYearEnd,\n",
    "                        'signRelation':signRelation,\n",
    "                        '_tempTypePair':_tempTypePair,\n",
    "                        'notesConflict':notesConflict\n",
    "                    },ignore_index=True)\n",
    "                    # create positive relation in which relationYearStart = conflictYearEnd (ToDo: check if plus 1 is Ok, it didn't work out for creating year slices, since 1682-1683 was empty), until the earliest year of death of one of the correspondents\n",
    "                    correspPairIds = rowB['correspPairIds']\n",
    "                    SKpersonId_person1 = rowB['SKpersonId_person1']\n",
    "                    personStrId_person1 = rowB['personStrId_person1']\n",
    "                    SKpersonId_person2 = rowB['SKpersonId_person2']\n",
    "                    personStrId_person2 = rowB['personStrId_person2']\n",
    "                    relationYearStart = rowA['conflictYearEnd']\n",
    "                    relationYearEnd = rowB['correspYearEnd']\n",
    "                    signRelation = 1\n",
    "                    _tempTypePair = 'slice1_certain'\n",
    "                    notesConflict = rowA['notesConflict']\n",
    "                    dfRelations1_v0 = dfRelations1_v0.append({\n",
    "                        'SKpersonId_person1':SKpersonId_person1,\n",
    "                        'personStrId_person1':personStrId_person1,\n",
    "                        'SKpersonId_person2':SKpersonId_person2,\n",
    "                        'personStrId_person2':personStrId_person2,\n",
    "                        'correspPairIds':correspPairIds,\n",
    "                        'relationYearStart':relationYearStart,\n",
    "                        'relationYearEnd':relationYearEnd,\n",
    "                        'signRelation':signRelation,\n",
    "                        '_tempTypePair':_tempTypePair,\n",
    "                        'notesConflict':notesConflict\n",
    "                    },ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f687fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first relations dataset\n",
    "dfRelations1_v0.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5544a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations1_v0.correspPairIds.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd82c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 200: check group in the corresponence set\n",
    "# check updates in relations\n",
    "test200 = combinedSetGroupsTies[combinedSetGroupsTies.correspPairIds.str.contains('skp003623@skp003755')] #with correspondence before conflict\n",
    "test200\n",
    "\n",
    "#skp003623@mop2088  with death date\n",
    "#skp003623@skp003755 Bassetti\n",
    "#skp024024@skp028586 Gronovius/Graevius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 20\n",
    "# check updates in relations\n",
    "test20 = dfRelations1_v0[dfRelations1_v0.correspPairIds.str.contains('skp003623@skp003755')] \n",
    "test20\n",
    "\n",
    "#skp003623@skp003755 Bassetti\n",
    "#skp024024@skp028586 Gronovius/Graevius\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test20.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a336128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 21\n",
    "# check updates in annotated file\n",
    "test21 = annotatedTiesA_v2[annotatedTiesA_v2.correspPairIds.str.contains('skp003623@skp003755')] \n",
    "test21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06307536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 22\n",
    "# check updates in annotated file\n",
    "test22 = combinedSetGroupsTies[combinedSetGroupsTies.correspPairIds.str.contains('skp024024@skp028586')] \n",
    "test22\n",
    "\n",
    "#skp008989@skp037757\n",
    "#skp003623@skp003755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a8c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "\n",
    "dfRelations1_v0_columns = dfRelations1_v0.columns\n",
    "\n",
    "for column in dfRelations1_v0_columns:\n",
    "    dataType = dfRelations1_v0.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        dfRelations1_v0[column] = dfRelations1_v0[column].fillna(0.0)\n",
    "        dfRelations1_v0[column] = dfRelations1_v0[column].astype(int)\n",
    "    if dataType == object:\n",
    "        dfRelations1_v0[column] = dfRelations1_v0[column].fillna('null')\n",
    "        dfRelations1_v0[column] = dfRelations1_v0[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421317c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the first relations dataset (*relations)\n",
    "dfRelations1 = dfRelations1_v0.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d382c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# check updates in annotated file\n",
    "test27 = dfRelations1[dfRelations1.correspPairIds.str.contains('skp024024@skp028586')] \n",
    "test27\n",
    "\n",
    "#skp024024@skp028586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when there are no other rows in the main aggregated set which have the same pair Id as in the annotated set, create a default positive relation in which relationYearStart = correspYearStart, \n",
    "# until correspYearEnd (by default, the assumption is that they had a positive relation during those exchanges)\n",
    "\n",
    "# First obtain the rows from the combined dataset which have no equivalent pair in the annotated dataset (using the correspPairIds)\n",
    "df_diff_pairs = combinedSetGroupsTies[~combinedSetGroupsTies.correspPairIds.isin(annotatedTiesA_v2.correspPairIds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_pairs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3623a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now create another dataframe with the necessary columns only (for the relationships) and add a default positive relation to those pairs\n",
    "dfRelations2_v0 = df_diff_pairs[['correspPairIds', 'SKpersonId_person1', 'personStrId_person1', 'SKpersonId_person2','personStrId_person2', '_tempTypePair', 'correspYearStart', 'correspYearEnd']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29eddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations2_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650aec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now create the sign relation column for that set, positive by default\n",
    "dfRelations2_v0['signRelation'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations2_v2 = dfRelations2_v0.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations2_v2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdcfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations2_v2.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because in this set the relations are only based on the ranges of the correspondence, we update the column names accordingly\n",
    "dfRelations2_v3 = dfRelations2_v2.rename(columns={'correspYearStart':'relationYearStart',\n",
    "                                                  'correspYearEnd':'relationYearEnd'\n",
    "                                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f53926",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations2_v4 = dfRelations2_v3.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations2_v4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes\n",
    "dfRelations2_v4['correspPairIds'] = dfRelations2_v4['correspPairIds'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b461a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the second relations dataset (*relations)\n",
    "dfRelations2 = dfRelations2_v4.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb99f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# check updates in annotated file\n",
    "test26 = dfRelations2[dfRelations2.correspPairIds.str.contains('skp010907@skp035864')] \n",
    "test26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c85511",
   "metadata": {},
   "source": [
    "## Extra dataB: annotated ties for non-existing pairs\n",
    "This file contains data of manually added pairs of correspondents and an annotated tie (negative or positive). It was compiled during archival research by Ingeborg van Vugt. These correspondent pairs do not exist in the aggregated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af2e35",
   "metadata": {},
   "source": [
    "### Upload dataset B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53160a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read annotated negative tiesB file\n",
    "negative_tiesB_path = os.path.join(data_raw_directory, 'extraPairsWithTiesB_20220414_182100.csv')\n",
    "\n",
    "annotatedTiesB_v00 = pd.read_csv(negative_tiesB_path, sep = \",\", index_col=False, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "\n",
    "annotatedTiesB_columns = annotatedTiesB_v00.columns\n",
    "\n",
    "for column in annotatedTiesB_columns:\n",
    "    dataType = annotatedTiesB_v00.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        annotatedTiesB_v00[column] = annotatedTiesB_v00[column].fillna(0.0)\n",
    "        annotatedTiesB_v00[column] = annotatedTiesB_v00[column].astype(int)\n",
    "    if dataType == object:\n",
    "        annotatedTiesB_v00[column] = annotatedTiesB_v00[column].fillna('null')\n",
    "        annotatedTiesB_v00[column] = annotatedTiesB_v00[column].astype(str)\n",
    "        \n",
    "# convert dates of birth/death to string to be able to concatenate them into a personStrId\n",
    "annotatedTiesB_v00['dateBirth_sender'] = annotatedTiesB_v00['dateBirth_sender'].astype(str)\n",
    "annotatedTiesB_v00['dateDeath_sender'] = annotatedTiesB_v00['dateDeath_sender'].astype(str)\n",
    "annotatedTiesB_v00['dateFl_sender'] = annotatedTiesB_v00['dateFl_sender'].astype(str)\n",
    "annotatedTiesB_v00['dateBirth_receiver'] = annotatedTiesB_v00['dateBirth_receiver'].astype(str)\n",
    "annotatedTiesB_v00['dateDeath_receiver'] = annotatedTiesB_v00['dateDeath_receiver'].astype(str)\n",
    "annotatedTiesB_v00['dateFl_receiver'] = annotatedTiesB_v00['dateFl_receiver'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a899805",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v01 = annotatedTiesB_v00.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v01.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7dd70",
   "metadata": {},
   "source": [
    "### Generate correspondent pair Ids\n",
    "Following the conventions in previous steps, the pair Id is created by joining the Ids of the correspondents when their names have been sorted alphabetically\n",
    "\n",
    "ToDo: perhaps create automatically the personStrId, to avoid possible duplicates..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612a814",
   "metadata": {},
   "source": [
    "#### Create personStrId\n",
    "To avoide duplicates in the dictionary due to equal strings (different dates = different persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v01['personStrId_sender'] = annotatedTiesB_v01['nameString_sender'] + '^' + annotatedTiesB_v01['dateBirth_sender'] + '^' + annotatedTiesB_v01['dateDeath_sender'] + '^' + annotatedTiesB_v01['dateFl_sender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v01['personStrId_receiver'] = annotatedTiesB_v01['nameString_receiver'] + '^' + annotatedTiesB_v01['dateBirth_receiver'] + '^' + annotatedTiesB_v01['dateDeath_receiver'] + '^' + annotatedTiesB_v01['dateFl_receiver']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058875e9",
   "metadata": {},
   "source": [
    "#### Create a dictionary of personId and person name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3344a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create dictionary for senders (source: https://cmdlinetips.com/2021/04/convert-two-column-values-from-pandas-dataframe-to-a-dictionary/)\n",
    "sendersB = annotatedTiesB_v01.set_index('SKpersonId_sender').to_dict()['personStrId_sender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9affea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sendersB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sendersB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9486a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: find Id per value using list comprehension\n",
    "dict_itemsB = sendersB.items()\n",
    "myValue3 = \"Ludolphus Kuster^1670^1716^0\"\n",
    "myKey3 = [key for key,value in dict_itemsB if value==myValue3]\n",
    "print(myKey3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30859d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for receivers\n",
    "receiversB = annotatedTiesB_v01.set_index('SKpersonId_receiver').to_dict()['personStrId_receiver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "receiversB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: find Id per value using list comprehension\n",
    "dict_itemsB = receiversB.items()\n",
    "myValue4 = \"Cosson Abraham^0^0^1701\"\n",
    "myKey4 = [key for key,value in dict_itemsB if value==myValue4]\n",
    "print(myKey4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ffb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(receiversB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c70dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both dictionaries (source: https://datagy.io/python-merge-dictionaries/)\n",
    "personsDictB = {**sendersB, **receiversB} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique persons in the combined dictionary\n",
    "len(personsDictB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923740ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Way to check that the dictionary doesn't contain duplicated values (source: https://www.geeksforgeeks.org/python-test-if-dictionary-contains-unique-keys-and-values/)\n",
    "# using loops\n",
    "# check for unique values\n",
    "flag = False\n",
    "hash_val = dict()\n",
    "for keys in sendersB:\n",
    "    if sendersB[keys] in hash_val:\n",
    "        flag = True\n",
    "        break\n",
    "    else :\n",
    "        hash_val[sendersB[keys]] = 1\n",
    "  \n",
    "# print result\n",
    "print(\"Does dictionary contain repetition : \" + str(flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values and keys\n",
    "key_listB = list(personsDictB.keys())\n",
    "value_listB = list(personsDictB.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b039e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return key for any value\n",
    "def get_key(val):\n",
    "    for key, value in personsDictB.items():\n",
    "         if val == value:\n",
    "             return key\n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd176fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: find Id per value using list comprehension\n",
    "dict_itemsB = personsDictB.items()\n",
    "myValue6 = \"Cosson Abraham^0^0^1701\"\n",
    "myKey6 = [key for key,value in dict_itemsB if value==myValue6]\n",
    "print(myKey6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v01.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v02 = annotatedTiesB_v01.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ee7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v02.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2589c1fb",
   "metadata": {},
   "source": [
    "#### Generate pair Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in annotatedTiesB_v02.iterrows():\n",
    "    #capture personId for sender\n",
    "    senderId = annotatedTiesB_v02.iloc[index,0]\n",
    "    #capture personId for receiver\n",
    "    receiverId = annotatedTiesB_v02.iloc[index,5]\n",
    "    #capture personStrId for sender\n",
    "    senderName = annotatedTiesB_v02.iloc[index,14]\n",
    "    #capture personStrId for receiver\n",
    "    receiverName = annotatedTiesB_v02.iloc[index,15]\n",
    "    #find the first one in alphabetical order\n",
    "    firstPairName = min(senderName,receiverName)\n",
    "    # Get the key (personId) for this pair name using the dictionary created in the previous step\n",
    "    IdFirstName = get_key(firstPairName)\n",
    "    #find the second one in alphabetical order\n",
    "    secondPairName = max(senderName,receiverName)\n",
    "    # Get the key (personId) for this pair name using the dictionary created in the previous step\n",
    "    IdSecondName = get_key(secondPairName)\n",
    "    #add the names to the dataframe\n",
    "    annotatedTiesB_v02.loc[index,'correspPairIds'] = IdFirstName + \"@\" + IdSecondName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e420866",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v02.correspPairIds.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d85aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that these pairIds do not occur indeed in the main dataset (this should result in an empty dataframe)\n",
    "df_diff_pairsB = annotatedTiesB_v02[annotatedTiesB_v02.correspPairIds.isin(combinedSetGroupsTies.correspPairIds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_pairsB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b975636",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v02.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names: since the direction of the relationship is not anymore based on \"sender\"/\"receiver\"\n",
    "annotatedTiesB_v03 = annotatedTiesB_v02.rename(columns={'SKpersonId_sender':'SKpersonId_person1',\n",
    "                                                      'SKpersonId_receiver':'SKpersonId_person2',\n",
    "                                                      'personStrId_sender':'personStrId_person1',\n",
    "                                                      'personStrId_receiver':'personStrId_person2',\n",
    "                                                      'dateBirth_sender':'dateBirth_person1',\n",
    "                                                      'dateDeath_sender':'dateDeath_person1', \n",
    "                                                      'dateFl_sender':'dateFl_person1',\n",
    "                                                      'dateBirth_receiver':'dateBirth_person2', \n",
    "                                                      'dateDeath_receiver':'dateDeath_person2', \n",
    "                                                      'dateFl_receiver':'dateFl_person2',\n",
    "                                                      'conflictYearStart': 'relationYearStart' #here also change the column name of conflict start to relation start\n",
    "                                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v04 = annotatedTiesB_v03.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v04.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v04.conflictYearEnd.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5be9ee",
   "metadata": {},
   "source": [
    "### Add end of conflict year to annotated dataset using rules\n",
    "Because there is no correspondence data attached to these pairs, we generate the relationships only using the rules above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39060dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate rows in the annotated ties file checking if there is a conflict ending year\n",
    "# In that case, and if the end year of the conflict is missing, fill in the earliest date of death\n",
    "# Note: because this dataset doesn't have any information related to correspondence, we update directly the relationships dataset\n",
    "\n",
    "for index, row in annotatedTiesB_v04.iterrows():\n",
    "    # check rule1 (if the end of the relation is unknown, we use earliest date of death as default and don't generate a positive relation if the correspondence )\n",
    "    if row['conflictYearEnd'] == 0:\n",
    "        # determine which is the earliest date of death when both persons have dateDeath\n",
    "        # check if both persons have dateDeath\n",
    "        if (row['dateDeath_person1'] != 0) & (row['dateDeath_person2'] != 0):\n",
    "            # check which is the earliest date of death\n",
    "            if row['dateDeath_person1'] <= row['dateDeath_person2']:\n",
    "                #add earliest date of death to annotatedTiesApersonDates as end of the relationship\n",
    "                annotatedTiesB_v04.at[index,'relationYearEnd'] = row['dateDeath_person1']\n",
    "            # check which is the earliest date of death\n",
    "            elif row['dateDeath_person2'] <= row['dateDeath_person1']:\n",
    "                #add earliest date of death to annotatedTiesApersonDates as end of the relationship\n",
    "                annotatedTiesB_v04.at[index,'relationYearEnd'] = row['dateDeath_person2']\n",
    "        # determine which is the earliest date of death when only person1 has dateDeath\n",
    "        # check if person1 has dateDeath and person2 has not\n",
    "        if (row['dateDeath_person1'] != 0) & (row['dateDeath_person2'] == 0):\n",
    "            #add earliest date of death to annotatedTiesApersonDates as end of the relationship\n",
    "            annotatedTiesB_v04.at[index,'relationYearEnd'] = row['dateDeath_person1']\n",
    "        # determine which is the earliest date of death when only person2 has dateDeath\n",
    "        # check if person1 has no dateDeath and person2 has it\n",
    "        if (row['dateDeath_person1'] == 0) & (row['dateDeath_person2'] != 0):\n",
    "            #add earliest date of death to annotatedTiesApersonDates as end of the relationship\n",
    "            annotatedTiesB_v04.at[index,'relationYearEnd'] = row['dateDeath_person2']\n",
    "        # elif nor person1/person2 have dateDeath\n",
    "        elif (row['dateDeath_person1'] == 0) & (row['dateDeath_person2'] == 0):\n",
    "        #add Magliabechi's dateDeath as end of conflict\n",
    "            annotatedTiesB_v04.at[index,'relationYearEnd'] = 1714\n",
    "    else:\n",
    "        annotatedTiesB_v04.at[index,'relationYearEnd'] = row['conflictYearEnd']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad57c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v04.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v04.relationYearEnd.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes for relationYearEnd\n",
    "\n",
    "annotatedTiesB_v04[\"relationYearEnd\"] = annotatedTiesB_v04[\"relationYearEnd\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v04.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c05a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v05 = annotatedTiesB_v04.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v05.relationYearEnd.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6eba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if persons don't have complete dates of birth/death, add date of death of Magliabechi as end of negative relation\n",
    "annotatedTiesB_v05['relationYearEnd']= annotatedTiesB_v05['relationYearEnd'].replace(0,1714)\n",
    "\n",
    "# ToDo: convert 0 to na\n",
    "# annotatedTiesB_v05['relationYearEnd'] = annotatedTiesB_v05['relationYearEnd'].fillna(1714)\n",
    "# annotatedTiesB_v05['relationYearEnd'] = annotatedTiesB_v05['relationYearEnd'].astype(int)\n",
    "\n",
    "# annotatedTiesB_v04.loc[annotatedTiesB_v04.relationYearEnd == 0,'relationYearEnd'] = 1714\n",
    "# annotatedTiesB_v04['relationYearEnd'] = annotatedTiesB_v04['relationYearEnd'].replace(0, 1714)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b78ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v05.relationYearEnd.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ToDoL: check all relations that end in 1714, that it makes sense with the dates of birth/death of the correspondents when one is empty\n",
    "checkDateEndRelation = annotatedTiesB_v05[annotatedTiesB_v05.relationYearEnd == 1714]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2de0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkDateEndRelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v06 = annotatedTiesB_v05.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column for indicating the certainty/uncertainty of these data\n",
    "annotatedTiesB_v06['_tempTypePair'] = 'slice1_certain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb9cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedTiesB_v06.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df with relevant columns only\n",
    "dfRelations3_v0 = annotatedTiesB_v06[['correspPairIds', 'SKpersonId_person1', 'personStrId_person1', 'SKpersonId_person2','personStrId_person2','relationYearStart', 'relationYearEnd', 'signRelation', '_tempTypePair', 'notesConflict']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the third relations set (*relations)\n",
    "dfRelations3 = dfRelations3_v0.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b95ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15602ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b381c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# check updates in annotated file\n",
    "test28 = dfRelations3[dfRelations3.correspPairIds.str.contains('skp024024@mop1526')] \n",
    "test28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d36446d",
   "metadata": {},
   "source": [
    "# Combine all relations into one set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e91ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eda9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations_v0 = pd.concat([dfRelations1, dfRelations2, dfRelations3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbd8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "dfRelations_v0['notesConflict'] = dfRelations_v0['notesConflict'].fillna('null')\n",
    "dfRelations_v0['notesConflict'] = dfRelations_v0['notesConflict'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the resulting dataset with all relations\n",
    "dfRelations = dfRelations_v0.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09094226",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ca8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test23 = dfRelations[dfRelations.correspPairIds.str.contains('skp024024@skp028586')]\n",
    "test23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export relations data\n",
    "\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# fileName30 = (f\"{pathRepository}magliabechiRelationsAll_{timestr}.csv\")\n",
    "# dfRelations.to_csv(fileName30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4fadf1",
   "metadata": {},
   "source": [
    "# Create slices per year\n",
    "Split relationship year range into individual years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7b5ce",
   "metadata": {},
   "source": [
    "\n",
    "This step will generate two datasets that contains the final correspondent pairs (i.e., correspondent pairs + year ranges) of the aggregated datasets of Garfagnini and CEN. These datasets are:\n",
    "\n",
    "- All pairs: it contains both the certain and uncertain slices generated in the previous steps (step 4)\n",
    "- Slice certain only: it contains\n",
    "contain the pairs which range is most certain (\"slice1_certain\" from the previous step) and only the pairs from the uncertain set (\"slice2_uncertain\" from previous step). These pairs were classified according to the certainty level of the letter metadata into two groups (column \"_tempTypePair\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2282577",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkNegative = dfRelations[['correspPairIds', 'relationYearStart', 'relationYearEnd']].copy()\n",
    "\n",
    "checkNegative['substraction'] = checkNegative['relationYearEnd'] - checkNegative['relationYearStart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82099e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkNegative.substraction.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c78bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test31 = checkNegative[(checkNegative.substraction >= -57) & (checkNegative.substraction < 0)]\n",
    "test31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf645e",
   "metadata": {},
   "source": [
    "## Certain slice only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323203ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelations._tempTypePair.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea75931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For obtaining the relationships per year, we will limit to use the \"certain\" relationships in this section\n",
    "dfRelationsCertain_v0 = dfRelations[dfRelations._tempTypePair == 'slice1_certain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsCertain_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d439fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsCertain = dfRelationsCertain_v0.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsCertain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcc8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop row that has the negative value (it's a mistake -14) ToDo: correct this in original CEN\n",
    "# dfRelationsPerYear = dfRelations.drop(dfRelations.index[[4575]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfRelationsPerYear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5794cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsPerYear_certain_v01 = dfRelationsCertain.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252776f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsPerYear_certain_v01.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f26f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for each year in the range between min and max years\n",
    "\n",
    "for index, row in dfRelationsPerYear_certain_v01.iterrows():\n",
    "    start = dfRelationsPerYear_certain_v01.iloc[index,5]\n",
    "    end = dfRelationsPerYear_certain_v01.iloc[index,6]\n",
    "    periods = end - (start)\n",
    "    year_ranges = pd.interval_range(start=start, end=end, periods=periods)\n",
    "    years = year_ranges\n",
    "    for year in years:\n",
    "        dfRelationsPerYear_certain_v01.loc[index, year] = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e02ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsPerYear_certain_v01.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fd5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsPerYear_certain_v02 = dfRelationsPerYear_certain_v01.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test32 = dfRelationsPerYear_certain_v01[dfRelationsPerYear_certain_v01.correspPairIds.str.contains('skp024024@skp028586')]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "test32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c4353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test32.loc[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To inspect if it got all years in columns\n",
    "\n",
    "# dfA_columns = dfA.columns\n",
    "\n",
    "# for column in dfA_columns:\n",
    "#     print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756393e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsPerYear_certain_v02 = dfRelationsPerYear_certain_v01.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba54ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: BECAUSE I COULDN'T SOLVE THE PROBLEM OF HANDLING THE LABELS OF THESE INTERVALS, \n",
    "# HERE I EXPORT THE FILE TO DO SOME CHANGES IN SUBLIME AND IMPORT AGAIN --ToDo: fix this...\n",
    "\n",
    "# store this file in /data/temp folder of the repository\n",
    "\n",
    "# this line will add the time stamp to the file name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# (this is a file with all the relations per year, in which the column labels are a two-year range, it's to fix externally for now)\n",
    "relationsPerYearCertain_file = (f\"MagliabechiCombinedSet_correspondentGroupsPerYear_certain_toFixColumnLabels_{timestr}.csv\")\n",
    "relationsPerYearCertain_path = os.path.join(data_temp_directory, relationsPerYearCertain_file)\n",
    "dfRelationsPerYear_certain_v02.to_csv(relationsPerYearCertain_path, sep='â‘„', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL STEP --> TO DO BY USER OUTSIDE THE NOTEBOOK\n",
    "# Process the file outside, in Sublime:\n",
    "## â‘„(\\()(\\d+)(, \\d+)(\\]) --> replace with â‘„$2\n",
    "## then replace the , (commas) for semicolomns ; (#ToDo Liliana: in the original files replace the commas)\n",
    "## then replace the separator â‘„ for commas\n",
    "## THEN SAVE THE FILE WITH NAME: MagliabechiCombinedSet_correspondentGroupsPerYear_CERTAIN.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport the file\n",
    "\n",
    "# Read fixed file\n",
    "correspGroupsCertain_perYear_path = os.path.join(data_temp_directory, 'MagliabechiCombinedSet_correspondentGroupsPerYear_CERTAIN.csv')\n",
    "\n",
    "correspGroupsCertain_perYear_imp = pd.read_csv(correspGroupsCertain_perYear_path, sep = \",\", index_col=False, engine='python')\n",
    "\n",
    "\n",
    "\n",
    "# previous versions\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_cy29_20220207-130457.csv\", sep = \"â‘„\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_cy29_20220209-111115.csv\", sep = \"â‘„\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_cy29_20220209-134800.csv\", sep = \"â‘„\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_20220411-195800.csv\", sep = \"â‘„\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_20220414-195200.csv\", sep = \"â‘„\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_20220419-134900.csv\", sep = \",\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathRepository}/data/processed/MagliabechiCombinedSet_correspondentGroupsPerYear_CERTAIN_20220503-155200.csv\", sep = \",\", index_col=False, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfB_t00 = correspGroupsCertain_perYear_imp.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfB_t00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3886f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "\n",
    "# # first handle this exception:\n",
    "# dfB_t01['dateFl'] = dfB_t01['dateFl'].fillna(0.0)\n",
    "# dfB_t01['dateFl'] = dfB_t01['dateFl'].astype(int)\n",
    "\n",
    "dfB_t00_columns = dfB_t00.columns\n",
    "\n",
    "for column in dfB_t00_columns:\n",
    "    dataType = dfB_t00.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        dfB_t00[column] = dfB_t00[column].fillna(0.0)\n",
    "        dfB_t00[column] = dfB_t00[column].astype(int)\n",
    "    if dataType == object:\n",
    "        dfB_t00[column] = dfB_t00[column].fillna('null')\n",
    "        dfB_t00[column] = dfB_t00[column].astype(str)\n",
    "        \n",
    "# Handle exceptions (I have to convert these to string to use int in the following loop)\n",
    "dfB_t00['relationYearStart'] = dfB_t00['relationYearStart'].astype(str)\n",
    "dfB_t00['relationYearEnd'] = dfB_t00['relationYearEnd'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfB_t00.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop unnecessary columns\n",
    "\n",
    "# dfB_t01 = dfB_t00.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slices should be only: 1663 til 1714\n",
    "dfB_t02 = dfB_t00.drop(['1611','1612','1613','1614','1615','1616','1617','1618','1619','1620','1621','1622','1623','1624','1625','1626','1627','1628','1629','1630','1631','1632','1633','1634','1635','1636','1637','1638','1639','1640','1641','1642','1643','1644','1645','1646','1647','1648','1649','1650','1651','1652','1653','1654','1655','1656','1657','1658','1659','1660','1661','1662','1715','1716','1717','1718','1719','1720','1721','1722','1723','1724','1725','1726','1727','1728','1729','1730','1731','1732','1733','1734','1735','1736','1737','1738','1739','1740','1741','1742','1743','1744','1745','1746','1747','1748','1749','1750','1751','1752','1753','1754','1755','1756','1757','1758','1759','1760','1761','1762','1763','1764','1765','1766','1767','1768','1769','1770','1771','1772','1773','1774','1775','1776','1777','1778','1779','1780','1781','1782','1783','1784','1785','1786','1787','1788','1789','1790','1791','1792','1793','1794','1795'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac83193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary: ToDo: fix this later in the R script, the column name is more clear as \"signRelation\" not just as sign but we\n",
    "# change it here manually now for testing the R script (April 19, 2022)\n",
    "\n",
    "dfB_t02.rename(columns={\"signRelation\":\"sign\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3dd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfB = dfB_t02.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfB.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfB.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfB[dfB['1692'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e84a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "for column in dfB.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnsYears = dfB_columns #ToDo: create a range here\n",
    "columnsYears = ['1667','1668','1669','1670','1671','1672','1673','1674','1675','1676','1677','1678','1679','1680','1681','1682','1683','1684','1685','1686','1687','1688','1689','1690','1691','1692','1693','1694','1695','1696','1697','1698','1663','1664','1665','1666','1699','1700','1701','1702','1703','1704','1705','1706','1707','1708','1709','1710','1711','1712','1713','1714']\n",
    "\n",
    "dataFrameNames = []\n",
    "dataFrames = []\n",
    "\n",
    "# create dataframe name\n",
    "for column in columnsYears:\n",
    "    dfName = (f\"{column}\")\n",
    "    dataFrameNames.append(dfName)\n",
    "    dfC = dfB[dfB[column] != 0]\n",
    "    dfD = dfC[['SKpersonId_person1', 'SKpersonId_person2', 'personStrId_person1', 'personStrId_person2', 'sign']]\n",
    "    dfE = dfD.reset_index(drop=True).copy()\n",
    "    dataFrames.append(dfE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901aca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = columnsYears.index('1692')\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b35be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75cde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f2fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrames[25].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66495386",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameNames[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831780dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFramesDictionary = dict(zip(dataFrameNames, dataFrames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(dataFramesDictionary.keys())\n",
    "value_list = list(dataFramesDictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bfe8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store these files in /data/processed folder of the repository\n",
    "\n",
    "# create path to the sub-folder to store files in the repository where the processed data is located\n",
    "data_processed_directory_perYearCertain = os.path.join(data_directory, 'processed/perYear_certain')\n",
    "\n",
    "# this line will add the time stamp to the file name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "for key, value in dataFramesDictionary.items():\n",
    "    perYear_certain_file = (f\"MagliabechiCombinedSet_{key}_{timestr}.csv\")\n",
    "    perYear_certain_path = os.path.join(data_processed_directory_perYearCertain, perYear_certain_file)\n",
    "    value.to_csv(perYear_certain_path, sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d013ab8",
   "metadata": {},
   "source": [
    "## ALL: including uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d40a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For obtaining the relationships per year, we will limit to use the \"certain\" relationships in this section\n",
    "dfRelationsAll_v0 = dfRelations.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsAll_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsAll = dfRelationsAll_v0.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsAll.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a757cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop row that has the negative value (it's a mistake -14) ToDo: correct this in original CEN\n",
    "dfRelationsAllPerYear = dfRelationsAll.drop(dfRelations.index[[4454, 4482, 4558, 4569, 4623]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsAllPerYear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d807f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsAllPerYear_v01 = dfRelationsAllPerYear.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb03fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsAllPerYear_v01.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for each year in the range between min and max years\n",
    "\n",
    "for index, row in dfRelationsAllPerYear_v01.iterrows():\n",
    "    start = dfRelationsAllPerYear_v01.iloc[index,5]\n",
    "    end = dfRelationsAllPerYear_v01.iloc[index,6]\n",
    "    periods = end - (start)\n",
    "    year_ranges = pd.interval_range(start=start, end=end, periods=periods)\n",
    "    years = year_ranges\n",
    "    for year in years:\n",
    "        dfRelationsAllPerYear_v01.loc[index, year] = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa231b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsAllPerYear_v01.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsAllPerYear_v02 = dfRelationsAllPerYear_v01.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb29021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test32 = dfRelationsAllPerYear_v02[dfRelationsAllPerYear_v02.correspPairIds.str.contains('skp024024@skp028586')]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "test32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7102663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test32.loc[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fd55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To inspect if it got all years in columns\n",
    "\n",
    "# dfA_columns = dfA.columns\n",
    "\n",
    "# for column in dfA_columns:\n",
    "#     print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelationsAllPerYear_v03 = dfRelationsAllPerYear_v02.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4117af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: BECAUSE I COULDN'T SOLVE THE PROBLEM OF HANDLING THE LABELS OF THESE INTERVALS, \n",
    "# HERE I EXPORT THE FILE TO DO SOME CHANGES IN SUBLIME AND IMPORT AGAIN --ToDo: fix this...\n",
    "\n",
    "# store this file in /data/temp folder of the repository\n",
    "\n",
    "# this line will add the time stamp to the file name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# (this is a file with all the relations per year, in which the column labels are a two-year range, it's to fix externally for now)\n",
    "relationsPerYearAll_file = (f\"MagliabechiCombinedSet_correspondentGroupsPerYear_All_toFixColumnLabels_{timestr}.csv\")\n",
    "relationsPerYearAll_path = os.path.join(data_temp_directory, relationsPerYearAll_file)\n",
    "dfRelationsAllPerYear_v03.to_csv(relationsPerYearAll_path, sep='â‘„', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL STEP --> TO DO BY USER OUTSIDE THE NOTEBOOK\n",
    "# Process the file outside, in Sublime:\n",
    "## â‘„(\\()(\\d+)(, \\d+)(\\]) --> replace with â‘„$2\n",
    "## then replace the , (commas) for semicolomns ; (#ToDo Liliana: in the original files replace the commas)\n",
    "## then replace the separator â‘„ for commas\n",
    "## THEN SAVE THE FILE WITH NAME: MagliabechiCombinedSet_correspondentGroupsPerYear_ALL.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport the file\n",
    "\n",
    "# Read fixed file\n",
    "correspGroupsAll_perYear_path = os.path.join(data_temp_directory, 'MagliabechiCombinedSet_correspondentGroupsPerYear_ALL.csv')\n",
    "\n",
    "dfM_t00 = pd.read_csv(correspGroupsAll_perYear_path, sep = \",\", index_col=False, engine='python')\n",
    "\n",
    "\n",
    "# previous versions\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_cy29_20220207-130457.csv\", sep = \"â‘„\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_cy29_20220209-111115.csv\", sep = \"â‘„\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_cy29_20220209-134800.csv\", sep = \"â‘„\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_20220411-195800.csv\", sep = \"â‘„\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_20220414-195200.csv\", sep = \"â‘„\", index_col=False, engine='python')\n",
    "# dfB_t00 = pd.read_csv(f\"{pathFolder}MagliabechiCombinedSet_correspondentGroupsPerYear_WithLabels_20220419-134900.csv\", sep = \",\", index_col=False, engine='python')\n",
    "# dfM_t00 = pd.read_csv(f\"{pathRepository}/data/processed/MagliabechiCombinedSet_correspondentGroupsPerYear_ALL_20220503-155200.csv\", sep = \",\", index_col=False, engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM_t00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM_t00.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6eac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "\n",
    "# # first handle this exception:\n",
    "# dfB_t01['dateFl'] = dfB_t01['dateFl'].fillna(0.0)\n",
    "# dfB_t01['dateFl'] = dfB_t01['dateFl'].astype(int)\n",
    "\n",
    "dfM_t00_columns = dfM_t00.columns\n",
    "\n",
    "for column in dfM_t00_columns:\n",
    "    dataType = dfM_t00.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        dfM_t00[column] = dfM_t00[column].fillna(0.0)\n",
    "        dfM_t00[column] = dfM_t00[column].astype(int)\n",
    "    if dataType == object:\n",
    "        dfM_t00[column] = dfM_t00[column].fillna('null')\n",
    "        dfM_t00[column] = dfM_t00[column].astype(str)\n",
    "        \n",
    "# Handle exceptions (I have to convert these to string to use int in the following loop)\n",
    "dfM_t00['relationYearStart'] = dfM_t00['relationYearStart'].astype(str)\n",
    "dfM_t00['relationYearEnd'] = dfM_t00['relationYearEnd'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM_t00.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop unnecessary columns\n",
    "\n",
    "# dfM_t01 = dfM_t00.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e23ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slices should be only: 1663 til 1714\n",
    "dfM_t02 = dfM_t00.drop(['1611','1612','1613','1614','1615','1616','1617','1618','1619','1620','1621','1622','1623','1624','1625','1626','1627','1628','1629','1630','1631','1632','1633','1634','1635','1636','1637','1638','1639','1640','1641','1642','1643','1644','1645','1646','1647','1648','1649','1650','1651','1652','1653','1654','1655','1656','1657','1658','1659','1660','1661','1662','1715','1716','1717','1718','1719','1720','1721','1722','1723','1724','1725','1726','1727','1728','1729','1730','1731','1732','1733','1734','1735','1736','1737','1738','1739','1740','1741','1742','1743','1744','1745','1746','1747','1748','1749','1750','1751','1752','1753','1754','1755','1756','1757','1758','1759','1760','1761','1762','1763','1764','1765','1766','1767','1768','1769','1770','1771','1772','1773','1774','1775','1776','1777','1778','1779','1780','1781','1782','1783','1784','1785','1786','1787','1788','1789','1790','1791','1792','1793','1794','1795'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary: ToDo: fix this later in the R script, the column name is more clear as \"signRelation\" not just as sign but we\n",
    "# change it here manually now for testing the R script (April 19, 2022)\n",
    "\n",
    "dfM_t02.rename(columns={\"signRelation\":\"sign\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM = dfM_t02.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcad3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dec0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM[dfM['1692'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "for column in dfM.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd847f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnsYears = dfB_columns #ToDo: create a range here\n",
    "columnsYears = ['1667','1668','1669','1670','1671','1672','1673','1674','1675','1676','1677','1678','1679','1680','1681','1682','1683','1684','1685','1686','1687','1688','1689','1690','1691','1692','1693','1694','1695','1696','1697','1698','1663','1664','1665','1666','1699','1700','1701','1702','1703','1704','1705','1706','1707','1708','1709','1710','1711','1712','1713','1714']\n",
    "\n",
    "dataFrameNames = []\n",
    "dataFrames = []\n",
    "\n",
    "# create dataframe name\n",
    "for column in columnsYears:\n",
    "    dfName = (f\"{column}\")\n",
    "    dataFrameNames.append(dfName)\n",
    "    dfN = dfM[dfM[column] != 0]\n",
    "    dfO = dfN[['SKpersonId_person1', 'SKpersonId_person2', 'personStrId_person1', 'personStrId_person2', 'sign']]\n",
    "    dfP = dfO.reset_index(drop=True).copy()\n",
    "    dataFrames.append(dfP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfed987",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = columnsYears.index('1692')\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b39e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrames[25].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2584aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameNames[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96acc7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFramesDictionary = dict(zip(dataFrameNames, dataFrames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(dataFramesDictionary.keys())\n",
    "value_list = list(dataFramesDictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a92c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store these files in /data/processed folder of the repository\n",
    "\n",
    "# create path to the sub-folder to store files in the repository where the processed data is located\n",
    "data_processed_directory_perYearALL = os.path.join(data_directory, 'processed/perYear_ALL')\n",
    "\n",
    "# this line will add the time stamp to the file name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "for key, value in dataFramesDictionary.items():\n",
    "    perYearAll_file = (f\"MagliabechiCombinedSet_{key}_{timestr}.csv\")\n",
    "    perYearAll_path = os.path.join(data_processed_directory_perYearALL, perYearAll_file)\n",
    "    value.to_csv(perYearAll_path, sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb99655",
   "metadata": {},
   "source": [
    "# ANALYSIS (R script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944df6bf",
   "metadata": {},
   "source": [
    "To continue with the analysis part, move to the Jupyter notebook that contains the steps (in R) to do the network analysis of the negative ties. The notebook is here:\n",
    "https://github.com/inge1211/unlinked_historical_structural_balance (pending to add DOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5c26b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
